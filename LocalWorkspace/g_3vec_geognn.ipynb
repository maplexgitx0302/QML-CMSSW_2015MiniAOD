{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic packages\n",
    "import os, time, random\n",
    "from itertools import product\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# data\n",
    "import awkward as ak\n",
    "import d_hep_data\n",
    "\n",
    "# qml\n",
    "import pennylane as qml\n",
    "from pennylane import numpy as np\n",
    "\n",
    "# pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# pytorch_lightning\n",
    "import lightning as L\n",
    "import lightning.pytorch as pl\n",
    "\n",
    "# pytorch_geometric\n",
    "import networkx as nx\n",
    "import torch_geometric.nn as geom_nn\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import MessagePassing\n",
    "\n",
    "# scipy\n",
    "from sklearn import metrics\n",
    "\n",
    "# wandb\n",
    "import wandb\n",
    "from lightning.pytorch.loggers import WandbLogger\n",
    "wandb.login()\n",
    "\n",
    "# reproducibility\n",
    "L.seed_everything(3020616)\n",
    "\n",
    "# faster calculation on GPU but less precision\n",
    "torch.set_float32_matmul_precision(\"medium\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configuration dictionary\n",
    "cf = {}\n",
    "cf[\"time\"]     = time.strftime(\"%Y%m%d_%H%M%S\", time.localtime())\n",
    "cf[\"wandb\"]    = True\n",
    "cf[\"project\"]  = \"g_3vec_geognn\"\n",
    "cf[\"rnd_seed\"] = None # to be determined by for loop\n",
    "\n",
    "# data infotmation\n",
    "cf[\"num_events\"]    = \"50000\"\n",
    "cf[\"sig_channel\"]   = \"ZprimeToZhToZinvhbb\"\n",
    "cf[\"bkg_channel\"]   = \"QCD_HT2000toInf\"\n",
    "cf[\"jet_type\"]      = \"fatjet\"\n",
    "cf[\"subjet_radius\"] = None # to be determined from [0.25, 0.5, 0.75]\n",
    "cf[\"cut_limit\"]     = (800, 1600)\n",
    "cf[\"bin\"]           = 8\n",
    "cf[\"num_bin_data\"]  = None # to be determined from [100, 200, 300]\n",
    "\n",
    "# traning configuration\n",
    "cf[\"num_train_ratio\"]   = 0.8\n",
    "cf[\"num_test_ratio\"]    = 0.2\n",
    "cf[\"batch_size\"]        = 64\n",
    "cf[\"num_workers\"]       = 0\n",
    "cf[\"max_epochs\"]        = 100\n",
    "cf[\"accelerator\"]       = \"cpu\"\n",
    "cf[\"fast_dev_run\"]      = False\n",
    "cf[\"log_every_n_steps\"] = cf[\"batch_size\"] // 2\n",
    "\n",
    "# model hyperparameters\n",
    "cf[\"loss_function\"]  = nn.BCEWithLogitsLoss()\n",
    "cf[\"optimizer\"]      = optim.Adam\n",
    "cf[\"learning_rate\"]  = 1E-3\n",
    "\n",
    "# 2PCNN hyperparameters\n",
    "cf[\"gnn_layers\"] = None # to be determined by grid search\n",
    "cf[\"mlp_layers\"] = None # to be determined by grid search"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JetGraphDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, sig_events, bkg_events, link_mode, **kwargs):\n",
    "        '''Add a \"_\" prefix if it is a fastjet feature'''\n",
    "        super().__init__()\n",
    "        # jet events\n",
    "        sig_events = self._pre_transformation(sig_events)\n",
    "        bkg_events = self._pre_transformation(bkg_events)\n",
    "        sig_graph_list = self._create_graph_list(sig_events, 1, link_mode, **kwargs)\n",
    "        bkg_graph_list = self._create_graph_list(bkg_events, 0, link_mode, **kwargs)\n",
    "\n",
    "        # count the number of training, and testing\n",
    "        num_data = cf[\"bin\"] * cf[\"num_bin_data\"]\n",
    "        assert len(sig_graph_list) >= num_data, f\"sig data not enough: {len(sig_graph_list)} < {num_data}\"\n",
    "        assert len(bkg_graph_list) >= num_data, f\"bkg data not enough: {len(bkg_graph_list)} < {num_data}\"\n",
    "        num_train = int(num_data * cf[\"num_train_ratio\"])\n",
    "        num_test  = int(num_data * cf[\"num_test_ratio\"])\n",
    "        print(f\"DataLog: {cf['sig_channel']} has {len(sig_graph_list)} events | {cf['bkg_channel']} has {len(bkg_graph_list)} events.\")\n",
    "        print(f\"Choose num_data for each channel to be {num_data} | Each channel  has num_train = {num_train}, num_test = {num_test}\")\n",
    "\n",
    "        # prepare dataset for dataloader\n",
    "        train_idx = num_train\n",
    "        test_idx  = num_train + num_test\n",
    "        self.train_dataset = self.sig_graph_list[:train_idx] + self.bkg_graph_list[:train_idx]\n",
    "        self.test_dataset  = self.sig_graph_list[train_idx:test_idx] + self.bkg_graph_list[train_idx:test_idx]\n",
    "        self.valid_dataset = self.test_dataset\n",
    "    \n",
    "    def _pre_transformation(events):\n",
    "        subjet_pt  = events[\"_pt\"] / events[\"pt\"]\n",
    "        subjet_eta = events[\"_delta_eta\"]\n",
    "        subjet_phi = events[\"_delta_phi\"]\n",
    "        events     = ak.zip([subjet_pt, subjet_eta, subjet_phi])\n",
    "        events     = events.to_list()\n",
    "        events     = [torch.tensor(events[i], dtype=torch.float32) for i in range(len(events))]\n",
    "        return events\n",
    "\n",
    "    def _create_graph_list(self, events, y, link_mode, **kwargs):\n",
    "        # create pytorch_geometric \"Data\" object\n",
    "        graph_list = []\n",
    "        for x in events:\n",
    "            x.requires_grad = False\n",
    "            if link_mode == \"fully_connected\":\n",
    "                edge_index = list(product(range(len(x)), range(len(x))))\n",
    "            elif link_mode == \"top_n_closest\":\n",
    "                edge_index = []\n",
    "                for i in range(len(x)):\n",
    "                    delta_eta = x[1] - x[1][i]\n",
    "                    delta_phi = x[2] - x[2][i]\n",
    "                    top_n_idx = torch.argsort(delta_eta**2 + delta_phi**2)\n",
    "                    edge_index += [[i, idx] for idx in top_n_idx[1:1+kwargs[\"n\"]]]\n",
    "            edge_index = torch.tensor(edge_index).transpose(0, 1)\n",
    "            graph_list.append(Data(x=x, edge_index=edge_index, y=y))\n",
    "        random.shuffle(graph_list)\n",
    "        return graph_list\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=cf[\"batch_size\"], num_workers=cf[\"num_workers\"],  shuffle=True)\n",
    "\n",
    "    def valid_dataloader(self):\n",
    "        return DataLoader(self.valid_dataset, batch_size=cf[\"batch_size\"], num_workers=cf[\"num_workers\"],  shuffle=False)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_dataset, batch_size=cf[\"batch_size\"], num_workers=cf[\"num_workers\"],  shuffle=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassicalMLP(nn.Module):\n",
    "    def __init__(self, in_channel, out_channel, hidden_channel, num_layers):\n",
    "        super().__init__()\n",
    "        if num_layers == 0:\n",
    "            self.net = nn.Linear(in_channel, out_channel)\n",
    "        else:\n",
    "            net = [nn.Linear(in_channel, hidden_channel), nn.ReLU()]\n",
    "            for _ in range(num_layers-2):\n",
    "                net += [nn.Linear(hidden_channel, hidden_channel), nn.ReLU()]\n",
    "            net += [nn.Linear(hidden_channel, out_channel)]\n",
    "            self.net = nn.Sequential(*net)\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class QuantumMLP(nn.Module):\n",
    "    def __init__(self, num_qubits, num_layers, num_reupload, measurements):\n",
    "        super().__init__()\n",
    "        # create a quantum MLP\n",
    "        @qml.qnode(qml.device('default.qubit', wires=num_qubits))\n",
    "        def circuit(inputs, weights):\n",
    "            for i in range(num_reupload):\n",
    "                qml.AngleEmbedding(features=inputs, wires=range(num_qubits), rotation='Y')\n",
    "                qml.StronglyEntanglingLayers(weights=weights[i], wires=range(num_qubits))\n",
    "            measurements_dict = {\"X\":qml.PauliX, \"Y\":qml.PauliY, \"Z\":qml.PauliZ}\n",
    "            return [qml.expval(measurements_dict[m[1]](wires=m[0])) for m in measurements]\n",
    "        # turn the quantum circuit into a torch layer\n",
    "        weight_shapes = {\"weights\":(num_reupload, num_layers, num_qubits, 3)}\n",
    "        net = [qml.qnn.TorchLayer(circuit, weight_shapes=weight_shapes)]\n",
    "        self.net = nn.Sequential(*net)\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classical2PCNNForwardMP(MessagePassing):\n",
    "    def __init__(self, num_features, num_layers, aggr):\n",
    "        super().__init__(aggr=aggr)\n",
    "        self.mp_phi = ClassicalMLP(\n",
    "            in_channel     = 2*num_features,\n",
    "            out_channel    = num_features,\n",
    "            hidden_channel = 2*num_features,\n",
    "            num_layers     = num_layers,\n",
    "            )\n",
    "        self.mp_gamma = ClassicalMLP(\n",
    "            in_channel     = 2*num_features, \n",
    "            out_channel    = num_features,\n",
    "            hidden_channel = 2*num_features,\n",
    "            num_layers     = num_layers,\n",
    "        )\n",
    "    def forward(self, x, edge_index):\n",
    "        return self.propagate(edge_index, x=x)\n",
    "    def message(self, x_i, x_j):\n",
    "        return self.mp_phi(torch.cat((x_i, x_j), dim=-1))\n",
    "    def update(self, aggr_out, x):\n",
    "        return self.mp_gamma(torch.cat((x, aggr_out), dim=-1))\n",
    "    \n",
    "class Quantum2PCQNNForwardMP(MessagePassing):\n",
    "    def __init__(self, num_features, num_layers, num_reupload, aggr):\n",
    "        super().__init__(aggr=aggr)\n",
    "        measurements = [[i, \"Z\"] for i in range(num_features)]\n",
    "        self.mp_phi = QuantumMLP(\n",
    "            num_qubits   = 2*num_features, \n",
    "            num_layers   = num_layers,\n",
    "            num_reupload = num_reupload,\n",
    "            measurements = measurements,\n",
    "            )\n",
    "        self.mp_gamma = ClassicalMLP(\n",
    "            in_channel     = 2*len(measurements), \n",
    "            out_channel    = num_features,\n",
    "            hidden_channel = 2*num_features,\n",
    "            num_layers     = num_layers,\n",
    "        )\n",
    "    def forward(self, x, edge_index):\n",
    "        return self.propagate(edge_index, x=x)\n",
    "    def message(self, x_i, x_j):\n",
    "        return self.mp_phi(torch.cat((x_i, x_j), dim=-1))\n",
    "    def update(self, aggr_out, x):\n",
    "        return self.mp_gamma(torch.cat((x, aggr_out), dim=-1))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
