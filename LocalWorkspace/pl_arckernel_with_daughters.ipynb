{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# system\n",
    "import os, glob\n",
    "import time, datetime\n",
    "from tqdm import tqdm\n",
    "\n",
    "# hep and toy model\n",
    "import toy\n",
    "import uproot\n",
    "import awkward as ak\n",
    "import hep_events, hep_buffer\n",
    "\n",
    "# qml\n",
    "import pennylane as qml\n",
    "import pennylane.numpy as np\n",
    "\n",
    "# pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# pytorch_lightning\n",
    "import pytorch_lightning as pl\n",
    "import torchmetrics\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "\n",
    "# wandb\n",
    "import wandb\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_name = \"wandb_arckernel\"\n",
    "jet_type = \"jet\"\n",
    "\n",
    "# whether use toy data and quantum model\n",
    "train_toy = False\n",
    "train_quantum = True\n",
    "enable_logger = True\n",
    "fast_dev_run = False\n",
    "if train_toy:\n",
    "    project_name = \"wandb_toy\"\n",
    "\n",
    "# hep data hyper-parameters\n",
    "data_config = {\n",
    "    \"num_events\":50000,\n",
    "    \"num_particles\":3,\n",
    "    \"cut\":f\"({jet_type}_pt>=500)&({jet_type}_pt<=1500)\",\n",
    "    \"signal_channel\":\"ZprimeToZhToZinvhbb\",\n",
    "    \"background_channel\":\"QCD_HT2000toInf\",\n",
    "}\n",
    "\n",
    "# hyper-parameters\n",
    "hyper_config = {\n",
    "    # general settings\n",
    "    \"random_seed\":0,\n",
    "    \"max_num_data\":2000,\n",
    "    \"train_ratio\":0.8,\n",
    "    \"valid_ratio\":0.2,\n",
    "\n",
    "    # constants\n",
    "    \"lr\":1E-3,\n",
    "    \"batch_size\":128,\n",
    "    \"num_workers\":12,\n",
    "\n",
    "    # functions\n",
    "    \"logits_threshold\":0.5,\n",
    "    \"loss_function\":F.binary_cross_entropy,\n",
    "    \"accuracy_function\":torchmetrics.Accuracy(task=\"binary\"),\n",
    "\n",
    "    # wandb\n",
    "    \"log_freq\":1,\n",
    "    }\n",
    "\n",
    "# pytorch_lightning trainer\n",
    "trainer_config = {\n",
    "    \"accelerator\":\"gpu\",\n",
    "    \"max_epochs\":100,\n",
    "    \"fast_dev_run\":fast_dev_run,\n",
    "    \"deterministic\":True,\n",
    "    \"log_every_n_steps\":1,\n",
    "    \"auto_scale_batch_size\":None,\n",
    "}\n",
    "\n",
    "# for trainer callbacks config (need to reinitiate callback for each run)\n",
    "callback_config = {\n",
    "    EarlyStopping:{\"monitor\":\"valid_loss\", \"min_delta\":0, \"mode\":\"min\", \"patience\":50, \"verbose\":True}\n",
    "}\n",
    "\n",
    "if train_toy:\n",
    "    hyper_config, trainer_config, ansatz_config = toy.set_config(hyper_config, trainer_config)\n",
    "else:\n",
    "    if jet_type == \"jet\":\n",
    "        input_dim = 3 + 3*data_config[\"num_particles\"] # pt eta phi\n",
    "    elif jet_type == \"fatjet\":\n",
    "        input_dim = 3 + 3*data_config[\"num_particles\"] + 5 # pt eta phi and n-subjettiness\n",
    "    # ansatz\n",
    "    ansatz_config = [\n",
    "        # input_dim, hidden_dim, hidden_layers, num_layers, num_reupload\n",
    "        (input_dim, 10*input_dim, 3, 0, 0),\n",
    "        (input_dim, 10*input_dim, 4, 0, 0),\n",
    "        (input_dim, 10*input_dim, 5, 0, 0),\n",
    "        (input_dim, 10*input_dim, 6, 0, 0),\n",
    "        (input_dim, 50*input_dim, 3, 0, 0),\n",
    "        (input_dim, 50*input_dim, 4, 0, 0),\n",
    "        (input_dim, 50*input_dim, 5, 0, 0),\n",
    "        (input_dim, 50*input_dim, 6, 0, 0),\n",
    "\n",
    "        (input_dim, 1*input_dim, 3, 1, 1),\n",
    "        (input_dim, 1*input_dim, 4, 1, 1),\n",
    "        (input_dim, 5*input_dim, 3, 1, 1),\n",
    "        (input_dim, 5*input_dim, 4, 1, 1),\n",
    "        (input_dim, 1*input_dim, 3, 1, 2),\n",
    "        (input_dim, 1*input_dim, 4, 1, 2),\n",
    "        (input_dim, 5*input_dim, 3, 1, 2),\n",
    "        (input_dim, 5*input_dim, 4, 1, 2),\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(channel):\n",
    "    num_events, num_particles, cut = data_config[\"num_events\"], data_config[\"num_particles\"], data_config[\"cut\"]\n",
    "    jet_parent = hep_buffer.load_data_buffer(channel, hep_buffer.get_parent_info, num_events, jet_type, cut)\n",
    "    if num_particles >= 1:\n",
    "        jet_daughter = hep_buffer.load_data_buffer(channel, hep_buffer.get_daughter_info, num_events, num_particles, jet_type, cut)\n",
    "        return torch.cat((jet_parent, jet_daughter), dim=1)\n",
    "    else:\n",
    "        return jet_parent\n",
    "\n",
    "class JetDataset(Dataset):\n",
    "    def __init__(self, signal_events, background_events, norm):\n",
    "        x = torch.cat((signal_events ,background_events), dim=0)\n",
    "        if norm: x = self.get_norm(x)\n",
    "        y = torch.cat((torch.ones((len(signal_events)), 1), torch.zeros((len(background_events)), 1)), dim=0)\n",
    "        x.requires_grad = False\n",
    "        y.requires_grad = False\n",
    "        self.x, self.y = x, y\n",
    "    def __getitem__(self, index):\n",
    "        return self.x[index], self.y[index]\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    def get_norm(self, events):\n",
    "        parent_pt_eta_phi   = events[:, :3].reshape(-1, 3, 1)\n",
    "        daughter_pt_eta_phi = events[:, -3*data_config[\"num_particles\"]:].reshape(-1, 3, data_config[\"num_particles\"])\n",
    "        parent_pt, parent_eta, parent_phi = parent_pt_eta_phi.transpose(0, 1)\n",
    "        daughter_pt, daughter_eta, daughter_phi = daughter_pt_eta_phi.transpose(0, 1)\n",
    "        pt_ratio, delta_eta, delta_phi = daughter_pt/torch.sum(daughter_pt**2), daughter_eta-parent_eta, daughter_phi-parent_phi\n",
    "        delta_r, cluster_radius = torch.sqrt(delta_eta**2 + delta_phi**2), 1\n",
    "        norm_pt  = (pt_ratio) * (delta_r / cluster_radius)\n",
    "        norm_eta = (delta_eta / delta_r)\n",
    "        norm_phi = (delta_phi / delta_r)\n",
    "        if not ((torch.abs(norm_pt) <= 1).all() and (torch.abs(norm_eta) <= 1).all() and  (torch.abs(norm_phi) <= 1).all()):\n",
    "            num_norm_pt  = torch.sum(torch.abs(norm_pt) > 1)\n",
    "            num_norm_eta = torch.sum(torch.abs(norm_eta) > 1)\n",
    "            num_norm_phi = torch.sum(torch.abs(norm_phi) > 1)\n",
    "            raise(ValueError(f\"Recieve value greater than pi in torch.asin() : (num_pt={num_norm_pt}, num_eta={num_norm_eta}, num_phi={num_norm_phi})\"))\n",
    "        else:\n",
    "            print(f\"Log(get_norm):Arguments (norms) of arcsin are within [-1, 1]\")\n",
    "        events = torch.cat((events, norm_pt, norm_eta, norm_phi), dim=-1)\n",
    "        return events\n",
    "\n",
    "class JetDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, norm):\n",
    "        super().__init__()\n",
    "        self.norm = norm\n",
    "        # get signal and background data\n",
    "        self.signal_events     = get_data(data_config[\"signal_channel\"])\n",
    "        self.background_events = get_data(data_config[\"background_channel\"])\n",
    "        # for auto batch finding\n",
    "        self.batch_size = hyper_config[\"batch_size\"]\n",
    "        self.num_workers = hyper_config[\"num_workers\"]\n",
    "        # determine num_train, num_valid, num_test\n",
    "        num_sig   = len(self.signal_events)\n",
    "        num_bkg   = len(self.background_events)\n",
    "        num_data  = min(num_sig, num_bkg, hyper_config[\"max_num_data\"])\n",
    "        num_train = int(hyper_config[\"train_ratio\"] * num_data)\n",
    "        num_valid = int(hyper_config[\"valid_ratio\"] * num_train)\n",
    "        num_test  = num_data - num_train\n",
    "        num_train = num_train - num_valid\n",
    "        self.num_train, self.num_valid, self.num_test = num_train, num_valid, num_test\n",
    "        print(f\"JetDataModule INFO: num_train = {num_train}, num_valid = {num_valid}, num_test = {num_test}\")\n",
    "\n",
    "    def setup(self, stage):\n",
    "        train_idx = self.num_train\n",
    "        valid_idx = self.num_train + self.num_valid\n",
    "        test_idx  = self.num_train + self.num_valid + self.num_test\n",
    "        if stage == \"fit\":\n",
    "            self.train_dataset = JetDataset(self.signal_events[:train_idx], self.background_events[:train_idx], norm=self.norm)\n",
    "            self.valid_dataset = JetDataset(self.signal_events[train_idx:valid_idx], self.background_events[train_idx:valid_idx], norm=self.norm)\n",
    "        elif stage == \"test\":\n",
    "            self.test_dataset  = JetDataset(self.signal_events[valid_idx:test_idx], self.background_events[valid_idx:test_idx], norm=self.norm)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size, num_workers=self.num_workers,  shuffle=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.valid_dataset, batch_size=self.batch_size, num_workers=self.num_workers)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_dataset, batch_size=self.batch_size, num_workers=self.num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classical Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassicalFNNModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, hidden_layers):\n",
    "        super().__init__()\n",
    "        if hidden_layers == 0:\n",
    "            net = [nn.Linear(input_dim, 1)]\n",
    "        else:\n",
    "            net = [nn.Linear(input_dim, hidden_dim), nn.ReLU()]\n",
    "            for _ in range(hidden_layers-1):\n",
    "                net += [nn.Linear(hidden_dim, hidden_dim)]\n",
    "                net += [nn.ReLU()]\n",
    "            net += [nn.Linear(hidden_dim, 1)]\n",
    "        net += [nn.Sigmoid()]\n",
    "        self.net = nn.Sequential(*net)\n",
    "    def forward(self, x):\n",
    "        y = self.net(x)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantum Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Encoding layers (ENC) and Variational Quantum Circuit (VQC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ENCDaughterNorm:\n",
    "    def __init__(self, num_particles):\n",
    "        self.num_particles = num_particles\n",
    "        self.num_qubits = 3 * num_particles\n",
    "    def __call__(self, inputs):\n",
    "        inputs = inputs.reshape((3, -1))\n",
    "        norm_pt, norm_eta, norm_phi = inputs\n",
    "        for ptc in range(self.num_particles):\n",
    "            qml.RY(2 * torch.asin(norm_pt[ptc]), wires=3*ptc)\n",
    "            qml.RY(2 * torch.asin(norm_pt[ptc]), wires=3*ptc+1)\n",
    "            qml.CRY(2 * torch.asin(norm_eta[ptc]), wires=[3*ptc, 3*ptc+2])\n",
    "            qml.CRY(2 * torch.asin(norm_phi[ptc]), wires=[3*ptc+1, 3*ptc+2])\n",
    "\n",
    "class VQCRotCNOT:\n",
    "    def __init__(self, num_qubits, num_layers):\n",
    "        self.num_qubits = num_qubits\n",
    "        self.num_layers = num_layers\n",
    "    def __call__(self, weights):\n",
    "        num_qubits = self.num_qubits\n",
    "        num_layers = self.num_layers\n",
    "        for l in range(num_layers):\n",
    "            for q in range(num_qubits):\n",
    "                qml.Rot(*weights[l][q], wires=q)\n",
    "            for q in range(num_qubits):\n",
    "                if q != num_qubits-1:\n",
    "                    qml.CNOT(wires=[q, q+1])\n",
    "                else:\n",
    "                    if num_qubits >= 3:\n",
    "                        qml.CNOT(wires=[q, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Quantum Layers and Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qml_torch_layer(enc_layer, vqc_layer, weight_shapes, num_reupload):\n",
    "    num_qubits = max(enc_layer.num_qubits, vqc_layer.num_qubits)\n",
    "    dev = qml.device('default.qubit', wires=num_qubits)\n",
    "    @qml.qnode(dev)\n",
    "    def qnode(inputs, weights):\n",
    "        for r in range(num_reupload):\n",
    "            enc_layer(inputs)\n",
    "            vqc_layer(weights[r])\n",
    "        return [qml.expval(qml.PauliZ(wires=i)) for i in range(num_qubits)]\n",
    "    return qml.qnn.TorchLayer(qnode, weight_shapes)\n",
    "\n",
    "\n",
    "class HybridArcKernelDaughterModel(nn.Module):\n",
    "    def __init__(self, classical_model, num_particles, num_layers, num_reupload):\n",
    "        super().__init__()\n",
    "        self.num_particles = num_particles\n",
    "        num_qubits = 3 * num_particles\n",
    "        self.num_qubits = num_qubits\n",
    "        weight_shapes = {\"weights\":(num_reupload, num_layers, num_qubits, 3)}\n",
    "        enc_layer = ENCDaughterNorm(num_particles)\n",
    "        vqc_layer = VQCRotCNOT(num_qubits, num_layers)\n",
    "        self.quantum_kernel = qml_torch_layer(enc_layer, vqc_layer, weight_shapes, num_reupload)\n",
    "        self.classical_model = classical_model\n",
    "    def forward(self, x):\n",
    "        num_particles = self.num_particles\n",
    "        norm_pt_eta_phi = x[:, -3*num_particles:].reshape(-1, 3, num_particles)\n",
    "        norm_pt, norm_eta, norm_phi = norm_pt_eta_phi[:, 0], norm_pt_eta_phi[:, 1], norm_pt_eta_phi[:, 2]\n",
    "        quantum_input = torch.cat((norm_pt, norm_eta, norm_phi), dim=1)\n",
    "        x = torch.cat((x[:, :-3*num_particles], self.quantum_kernel(quantum_input)), dim=1)\n",
    "        y = self.classical_model(x)\n",
    "        return y"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LightningModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LitModel(pl.LightningModule):\n",
    "    def __init__(self, model, *args, **kwargs):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters(ignore=['model'])\n",
    "        self.model = model\n",
    "        self.loss_func = hyper_config[\"loss_function\"]\n",
    "        self.acc_func = hyper_config[\"accuracy_function\"]\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self.model(x)\n",
    "        train_loss = self.loss_func(logits, y) # note the order of arg of loss func\n",
    "        train_acc = self.acc_func(y, logits > hyper_config[\"logits_threshold\"])\n",
    "        self.log(\"train_loss\", train_loss, on_step=False, on_epoch=True)\n",
    "        self.log(\"train_acc\", train_acc, on_step=False, on_epoch=True)\n",
    "        return train_loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self.model(x)\n",
    "        valid_loss = self.loss_func(logits, y) # note the order of arg of loss func\n",
    "        valid_acc = self.acc_func(y, logits > hyper_config[\"logits_threshold\"])\n",
    "        self.log(\"valid_loss\", valid_loss, on_step=False, on_epoch=True)\n",
    "        self.log(\"valid_acc\", valid_acc, on_step=False, on_epoch=True)\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self.model(x)\n",
    "        test_loss = self.loss_func(logits, y) # note the order of arg of loss func\n",
    "        test_acc = self.acc_func(y, logits > hyper_config[\"logits_threshold\"])\n",
    "        self.log(\"test_loss\", test_loss, on_step=False, on_epoch=True)\n",
    "        self.log(\"test_acc\", test_acc, on_step=False, on_epoch=True)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=hyper_config[\"lr\"])\n",
    "        return optimizer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, datamodule, logger_name):\n",
    "    pl.seed_everything(hyper_config[\"random_seed\"], workers=True)\n",
    "\n",
    "    # pl logger setup\n",
    "    logger = None\n",
    "    if enable_logger:\n",
    "        logger = WandbLogger(project=project_name, name=f\"{logger_name}\", id=logger_name)\n",
    "        wandb_config = {}\n",
    "        wandb_config.update(data_config)\n",
    "        wandb_config.update(hyper_config)\n",
    "        wandb_config.update(trainer_config)\n",
    "        logger.experiment.config.update(wandb_config, allow_val_change=True)\n",
    "        logger.watch(model, log=\"all\", log_freq=hyper_config[\"log_freq\"])\n",
    "    \n",
    "    # pl callbacks setup\n",
    "    callbacks = []\n",
    "    for key, value in callback_config.items():\n",
    "        callbacks.append(key(**value))\n",
    "\n",
    "    # trainer setup\n",
    "    trainer = pl.Trainer(**trainer_config, logger=logger, callbacks=callbacks)\n",
    "    if trainer_config[\"auto_scale_batch_size\"] != None:\n",
    "        trainer.tune(model, datamodule)\n",
    "    \n",
    "    # start fitting and testing\n",
    "    checkpoints_dir = f\"./{project_name}/{logger_name}/checkpoints/\"\n",
    "    try:\n",
    "        ckpt_path = glob.glob(f'{checkpoints_dir}/*ckpt')[0]\n",
    "    except:\n",
    "        ckpt_path = None\n",
    "        print(f\"Log(checkpoints): *.ckpt files not found, start training from initial state.\")\n",
    "    trainer.fit(model=model, datamodule=datamodule, ckpt_path=ckpt_path)\n",
    "    trainer.test(model=model, datamodule=datamodule)\n",
    "    if enable_logger:\n",
    "        wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data modules\n",
    "pl.seed_everything(hyper_config[\"random_seed\"], workers=True)\n",
    "if train_toy:\n",
    "    c_datamodule = toy.ToyDataModule(batch_size=hyper_config[\"batch_size\"], num_workers=hyper_config[\"num_workers\"])\n",
    "    q_datamodule = toy.ToyDataModule(batch_size=hyper_config[\"batch_size\"], num_workers=hyper_config[\"num_workers\"])\n",
    "else:\n",
    "    c_datamodule = JetDataModule(norm=False)\n",
    "    q_datamodule = JetDataModule(norm=True)\n",
    "\n",
    "if train_toy:\n",
    "    for ansatz in tqdm(ansatz_config, \"Toy Model\"):\n",
    "        logger_name = f\"toy_({','.join(list(map(str, ansatz)))})\"\n",
    "        model = LitModel(ClassicalFNNModel(*ansatz))\n",
    "        train(model, c_datamodule, logger_name)\n",
    "else:\n",
    "    for ansatz in tqdm(ansatz_config, \"HEP Model\"):\n",
    "        c_model_class, q_model_class = ClassicalFNNModel, HybridArcKernelDaughterModel\n",
    "        logger_prefix = f\"{jet_type}_ptc{data_config['num_particles']}_n{hyper_config['max_num_data']}_{data_config['cut']}\"\n",
    "        c_logger_name = f\"{logger_prefix}_classical_{c_model_class.__name__}_({','.join(list(map(str, ansatz)))})\"\n",
    "        c_model = LitModel(c_model_class(*ansatz[:3]))\n",
    "        train(c_model, c_datamodule, c_logger_name)\n",
    "        if train_quantum and (ansatz[3] != 0 or ansatz[4] != 0):\n",
    "            q_logger_name = f\"{logger_prefix}_quantum_{q_model_class.__name__}_({','.join(list(map(str, ansatz)))})\"\n",
    "            q_ansatz = list(ansatz)\n",
    "            q_ansatz[0] += 3 * data_config[\"num_particles\"]\n",
    "            c_model = ClassicalFNNModel(*q_ansatz[:3])\n",
    "            \n",
    "            q_model = LitModel(q_model_class(c_model, data_config['num_particles'], *q_ansatz[3:]))\n",
    "            train(q_model, q_datamodule, q_logger_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit ('3.9.12')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "abdd0d95ca50f233d1202cce1ba28eab5ada50f7ec17823ef40ef9b79347f6f5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
