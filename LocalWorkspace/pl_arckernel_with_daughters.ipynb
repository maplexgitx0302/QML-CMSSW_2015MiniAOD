{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yianchen/.pyenv/versions/3.9.12/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# system\n",
    "import os\n",
    "\n",
    "# hep\n",
    "import uproot\n",
    "import awkward as ak\n",
    "import hep_events, hep_buffer\n",
    "\n",
    "# qml\n",
    "import pennylane as qml\n",
    "import pennylane.numpy as np\n",
    "\n",
    "# pytorch and lightning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pytorch_lightning as pl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hep hyper-parameters\n",
    "jet_type   = \"fatjet\"\n",
    "num_events = 50000\n",
    "num_particles = 3\n",
    "cut = f\"({jet_type}_pt >= 900) & ({jet_type}_pt <= 1100)\"\n",
    "signal_channel = \"ZprimeToZhToZinvhbb\"\n",
    "# signal_channel = \"ZprimeToZhToZlephbb\"\n",
    "# background_channel = \"QCD_HT1500to2000\"\n",
    "background_channel = \"QCD_HT2000toInf\"\n",
    "\n",
    "# training hyper-parameters\n",
    "max_num_data = 2000\n",
    "train_ratio = 0.8\n",
    "valid_ratio = 0.2\n",
    "\n",
    "config = {\n",
    "    \"lr\":1E-3,\n",
    "    \"num_epochs\":5,\n",
    "    \"batch_size\":32,\n",
    "    \"loss_function\":nn.BCEWithLogitsLoss(reduction=\"mean\"),\n",
    "    \"fast_dev_run\":False,\n",
    "    \"num_workers\":0,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log(load_data_buffer): ZprimeToZhToZinvhbb buffer found, loading complete!\n",
      "Log(load_data_buffer): ZprimeToZhToZinvhbb buffer found, loading complete!\n",
      "Log(load_data_buffer): QCD_HT2000toInf buffer found, loading complete!\n",
      "Log(load_data_buffer): QCD_HT2000toInf buffer found, loading complete!\n",
      "----------------------------------------------------------------------------------------------------\n",
      "- Cut = (fatjet_pt >= 900) & (fatjet_pt <= 1100)\n",
      "- Signal = ZprimeToZhToZinvhbb(num=4852)\n",
      "- Background = QCD_HT2000toInf(num=30790)\n",
      "- Choose number of data = 2000(max=2000)\n",
      "- Train=1280 | Valid=320 | Test=400\n",
      "- Shape of data = torch.Size([4852, 15])\n"
     ]
    }
   ],
   "source": [
    "def get_data(channel, num_events, num_particles, jet_type, cut):\n",
    "    jet_parent = hep_buffer.load_data_buffer(channel, hep_buffer.get_parent_info, num_events, jet_type, cut)\n",
    "    if num_particles >= 1:\n",
    "        jet_daughter = hep_buffer.load_data_buffer(channel, hep_buffer.get_daughter_info, num_events, num_particles, jet_type, cut)\n",
    "        return torch.cat((jet_parent, jet_daughter), dim=1)\n",
    "    else:\n",
    "        return jet_parent\n",
    "\n",
    "signal_events = get_data(signal_channel, num_events, num_particles, jet_type, cut)\n",
    "background_events = get_data(background_channel, num_events, num_particles, jet_type, cut)\n",
    "num_sig, num_bkg = len(signal_events), len(background_events)\n",
    "num_data = min(num_sig, num_bkg, max_num_data)\n",
    "num_train = int(train_ratio * num_data)\n",
    "num_valid = int(valid_ratio * num_train)\n",
    "num_test = num_data - num_train\n",
    "num_train = num_train - num_valid\n",
    "print(\"-\" * 100)\n",
    "print(f\"- Cut = {cut}\")\n",
    "print(f\"- Signal = {signal_channel}(num={num_sig})\")\n",
    "print(f\"- Background = {background_channel}(num={num_bkg})\")\n",
    "print(f\"- Choose number of data = {num_data}(max={max_num_data})\")\n",
    "print(f\"- Train={num_train} | Valid={num_valid} | Test={num_test}\")\n",
    "print(f\"- Shape of data = {signal_events.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JetDataset(Dataset):\n",
    "    def __init__(self, signal_events, background_events, norm):\n",
    "        x = torch.cat((signal_events ,background_events), dim=0)\n",
    "        if norm: x = self.get_norm(x)\n",
    "        y = torch.cat((torch.ones((len(signal_events)), 1), torch.zeros((len(background_events)), 1)), dim=0)\n",
    "        x.requires_grad = False\n",
    "        y.requires_grad = False\n",
    "        self.x, self.y = x, y\n",
    "    def __getitem__(self, index):\n",
    "        return self.x[index], self.y[index]\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    def get_norm(self, events):\n",
    "        parent_pt_eta_phi   = events[:, :3].reshape(-1, 3, 1)\n",
    "        daughter_pt_eta_phi = events[:, -3*num_particles:].reshape(-1, 3, num_particles)\n",
    "        parent_pt, parent_eta, parent_phi = parent_pt_eta_phi.transpose(0, 1)\n",
    "        daughter_pt, daughter_eta, daughter_phi = daughter_pt_eta_phi.transpose(0, 1)\n",
    "        pt_ratio, delta_eta, delta_phi = daughter_pt/parent_pt, daughter_eta-parent_eta, daughter_phi-parent_phi\n",
    "        delta_r, cluster_radius = torch.sqrt(delta_eta**2 + delta_phi**2), 1\n",
    "        norm_pt  = (1/daughter_pt) * delta_r / cluster_radius\n",
    "        norm_eta = delta_eta / delta_r\n",
    "        norm_phi = delta_phi / delta_r\n",
    "        if not ((torch.abs(norm_pt) <= 1).all() and (torch.abs(norm_eta) <= 1).all() and  (torch.abs(norm_phi) <= 1).all()):\n",
    "            num_norm_pt  = torch.sum(torch.abs(norm_pt) > 1)\n",
    "            num_norm_eta = torch.sum(torch.abs(norm_eta) > 1)\n",
    "            num_norm_phi = torch.sum(torch.abs(norm_phi) > 1)\n",
    "            print(f\"Recieve value greater than 1 in torch.asin() : (num_pt={num_norm_pt}, num_eta={num_norm_eta}, num_phi={num_norm_phi})\")\n",
    "            if num_norm_pt > 0:\n",
    "                norm_pt[norm_pt > 1] = 1\n",
    "                norm_pt[norm_pt < -1] = -1\n",
    "            if num_norm_eta > 0:\n",
    "                norm_eta[norm_eta > 1] = 1\n",
    "                norm_eta[norm_eta < -1] = -1\n",
    "            if num_norm_phi > 0:\n",
    "                norm_phi[norm_phi > 1] = 1\n",
    "                norm_phi[norm_phi < -1] = -1\n",
    "        events = torch.cat((events, norm_pt, norm_eta, norm_phi), dim=-1)\n",
    "        return events\n",
    "\n",
    "# classical dataset\n",
    "c_train_dataset = JetDataset(signal_events[:num_train], background_events[:num_train], norm=False)\n",
    "c_valid_dataset = JetDataset(signal_events[num_train:num_train+num_valid], background_events[num_train:num_train+num_valid], norm=False)\n",
    "c_test_dataset = JetDataset(signal_events[num_train+num_valid:num_data], background_events[num_train+num_valid:num_data], norm=False)\n",
    "\n",
    "# quantum dataset\n",
    "norm = num_particles > 0\n",
    "q_train_dataset = JetDataset(signal_events[:num_train], background_events[:num_train], norm)\n",
    "q_valid_dataset = JetDataset(signal_events[num_train:num_train+num_valid], background_events[num_train:num_train+num_valid], norm)\n",
    "q_test_dataset = JetDataset(signal_events[num_train:num_data], background_events[num_train:num_data], norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classical Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassicalFNNModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, hidden_layers):\n",
    "        super().__init__()\n",
    "        if hidden_layers == 0:\n",
    "            net = [nn.Linear(input_dim, 1)]\n",
    "        else:\n",
    "            net = [nn.Linear(input_dim, hidden_dim), nn.ReLU()]\n",
    "            for _ in range(hidden_layers-1):\n",
    "                net += [nn.Linear(hidden_dim, hidden_dim)]\n",
    "                net += [nn.ReLU()]\n",
    "            net += [nn.Linear(hidden_dim, 1)]\n",
    "        # BCEWithLogitsLoss already contains a sigmoid function\n",
    "        self.net = nn.Sequential(*net)\n",
    "    def forward(self, x):\n",
    "        y = self.net(x)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantum Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Encoding layers (ENC) and Variational Quantum Circuit (VQC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ENCDaughterNorm:\n",
    "    def __init__(self, num_particles):\n",
    "        self.num_particles = num_particles\n",
    "        self.num_qubits = 3 * num_particles\n",
    "    def __call__(self, inputs):\n",
    "        inputs = inputs.reshape((3, -1))\n",
    "        norm_pt, norm_eta, norm_phi = inputs\n",
    "        for ptc in range(self.num_particles):\n",
    "            qml.RY(2 * torch.asin(norm_pt[ptc]), wires=3*ptc)\n",
    "            qml.RY(2 * torch.asin(norm_pt[ptc]), wires=3*ptc+1)\n",
    "            qml.CRY(2 * torch.asin(norm_eta[ptc]), wires=[3*ptc, 3*ptc+2])\n",
    "            qml.CRY(2 * torch.asin(norm_phi[ptc]), wires=[3*ptc+1, 3*ptc+2])\n",
    "\n",
    "class VQCRotCNOT:\n",
    "    def __init__(self, num_qubits, num_layers):\n",
    "        self.num_qubits = num_qubits\n",
    "        self.num_layers = num_layers\n",
    "    def __call__(self, weights):\n",
    "        num_qubits = self.num_qubits\n",
    "        num_layers = self.num_layers\n",
    "        for l in range(num_layers):\n",
    "            for q in range(num_qubits):\n",
    "                qml.Rot(*weights[l][q], wires=q)\n",
    "            for q in range(num_qubits):\n",
    "                if q != num_qubits-1:\n",
    "                    qml.CNOT(wires=[q, q+1])\n",
    "                else:\n",
    "                    if num_qubits >= 3:\n",
    "                        qml.CNOT(wires=[q, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Quantum Layers and Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qml_torch_layer(enc_layer, vqc_layer, weight_shapes, num_reupload):\n",
    "    num_qubits = max(enc_layer.num_qubits, vqc_layer.num_qubits)\n",
    "    dev = qml.device('default.qubit', wires=num_qubits)\n",
    "    @qml.qnode(dev)\n",
    "    def qnode(inputs, weights):\n",
    "        for r in range(num_reupload):\n",
    "            enc_layer(inputs)\n",
    "            vqc_layer(weights[r])\n",
    "        return [qml.expval(qml.PauliZ(wires=i)) for i in range(num_qubits)]\n",
    "    return qml.qnn.TorchLayer(qnode, weight_shapes)\n",
    "\n",
    "\n",
    "class HybridArcKernelDaughterModel(nn.Module):\n",
    "    def __init__(self, classical_model, num_particles, num_layers, num_reupload):\n",
    "        super().__init__()\n",
    "        self.num_particles = num_particles\n",
    "        num_qubits = 3 * num_particles\n",
    "        self.num_qubits = num_qubits\n",
    "        weight_shapes = {\"weights\":(num_reupload, num_layers, num_qubits, 3)}\n",
    "        enc_layer = ENCDaughterNorm(num_particles)\n",
    "        vqc_layer = VQCRotCNOT(num_qubits, num_layers)\n",
    "        self.quantum_kernel = qml_torch_layer(enc_layer, vqc_layer, weight_shapes, num_reupload)\n",
    "        self.classical_model = classical_model\n",
    "    def forward(self, x):\n",
    "        num_particles = self.num_particles\n",
    "        norm_pt_eta_phi = x[:, -3*num_particles:].reshape(-1, 3, num_particles)\n",
    "        norm_pt, norm_eta, norm_phi = norm_pt_eta_phi[:, 0], norm_pt_eta_phi[:, 1], norm_pt_eta_phi[:, 2]\n",
    "        quantum_input = torch.cat((norm_pt, norm_eta, norm_phi), dim=1)\n",
    "        x = torch.cat((x[:, :-3*num_particles], self.quantum_kernel(quantum_input)), dim=1)\n",
    "        y = self.classical_model(x)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LitModel(pl.LightningModule):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "\n",
    "    def training_step(self, batch):\n",
    "        x, y_true = batch\n",
    "        y_pred = self.model(x)\n",
    "        loss = config[\"loss_function\"](y_true, y_pred)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=config[\"lr\"])\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
      "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
      "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
      "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
      "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "INFO:pytorch_lightning.callbacks.model_summary:\n",
      "  | Name  | Type                         | Params\n",
      "-------------------------------------------------------\n",
      "0 | model | HybridArcKernelDaughterModel | 2.3 M \n",
      "-------------------------------------------------------\n",
      "2.3 M     Trainable params\n",
      "0         Non-trainable params\n",
      "2.3 M     Total params\n",
      "9.162     Total estimated model params size (MB)\n",
      "/home/yianchen/.pyenv/versions/3.9.12/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 80/80 [01:23<00:00,  1.05s/it, loss=-1.48e+08, v_num=15]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_lightning.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=5` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 80/80 [01:23<00:00,  1.05s/it, loss=-1.48e+08, v_num=15]\n"
     ]
    }
   ],
   "source": [
    "# classical\n",
    "input_dim = signal_events.shape[1]\n",
    "hidden_dim, hidden_layers = 100 * input_dim, 2\n",
    "\n",
    "# quantum\n",
    "num_reupload = 1\n",
    "num_layers = 1\n",
    "\n",
    "c_train_loader = DataLoader(c_train_dataset, config[\"batch_size\"], shuffle=True, drop_last=True, num_workers=config[\"num_workers\"])\n",
    "q_train_loader = DataLoader(q_train_dataset, config[\"batch_size\"], shuffle=True, drop_last=True, num_workers=config[\"num_workers\"])\n",
    "\n",
    "c_nn = ClassicalFNNModel(input_dim, hidden_dim, hidden_layers)\n",
    "c_model = LitModel(c_nn)\n",
    "\n",
    "q_c_nn = ClassicalFNNModel(input_dim + 3*num_particles, hidden_dim, hidden_layers)\n",
    "q_nn = HybridArcKernelDaughterModel(q_c_nn, num_particles, num_layers, num_reupload)\n",
    "q_model = LitModel(q_nn)\n",
    "\n",
    "# c_trainer = pl.Trainer(accelerator=\"gpu\", max_epochs=config[\"num_epochs\"], fast_dev_run=config[\"fast_dev_run\"], log_every_n_steps=5)\n",
    "# c_trainer.fit(model=c_model, train_dataloaders=c_train_loader)\n",
    "q_trainer = pl.Trainer(accelerator=\"gpu\", max_epochs=config[\"num_epochs\"], fast_dev_run=config[\"fast_dev_run\"], log_every_n_steps=5)\n",
    "q_trainer.fit(model=q_model, train_dataloaders=q_train_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit ('3.9.12')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "abdd0d95ca50f233d1202cce1ba28eab5ada50f7ec17823ef40ef9b79347f6f5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
