{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# system\n",
    "import os\n",
    "\n",
    "# hep\n",
    "import uproot\n",
    "import awkward as ak\n",
    "import hep_events, hep_buffer\n",
    "\n",
    "# qml\n",
    "import pennylane as qml\n",
    "import pennylane.numpy as np\n",
    "\n",
    "# pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# pytorch_lightning\n",
    "import pytorch_lightning as pl\n",
    "import torchmetrics\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hep data hyper-parameters\n",
    "jet_type = \"fatjet\"\n",
    "data_config = {\n",
    "    \"num_events\":50000,\n",
    "    \"num_particles\":3,\n",
    "    \"cut\":f\"({jet_type}_pt >= 900) & ({jet_type}_pt <= 1100)\",\n",
    "    \"signal_channel\":\"ZprimeToZhToZinvhbb\",\n",
    "    \"background_channel\":\"QCD_HT2000toInf\",\n",
    "}\n",
    "\n",
    "# hyper-parameters\n",
    "hyper_config = {\n",
    "    # general settings\n",
    "    \"random_seed\":0,\n",
    "    \"max_num_data\":2000,\n",
    "    \"train_ratio\":0.8,\n",
    "    \"valid_ratio\":0.2,\n",
    "\n",
    "    # constants\n",
    "    \"lr\":1E-3,\n",
    "    \"batch_size\":32,\n",
    "    \"num_workers\":0,\n",
    "\n",
    "    # functions\n",
    "    \"loss_function\":nn.BCEWithLogitsLoss(reduction=\"mean\"),\n",
    "    \"accuracy_function\":torchmetrics.Accuracy(task=\"binary\"),\n",
    "    }\n",
    "\n",
    "# pytorch_lightning trainer\n",
    "trainer_config = {\n",
    "    \"accelerator\":\"gpu\",\n",
    "    \"max_epochs\":5,\n",
    "    \"fast_dev_run\":True,\n",
    "    \"deterministic\":True,\n",
    "    \"callbacks\":[EarlyStopping(monitor=\"valid_acc\", min_delta=1, mode=\"max\", patience=3)],\n",
    "}\n",
    "\n",
    "pl.seed_everything(hyper_config[\"random_seed\"], workers=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(channel):\n",
    "    num_events, num_particles, cut = data_config[\"num_events\"], data_config[\"num_particles\"], data_config[\"cut\"]\n",
    "    jet_parent = hep_buffer.load_data_buffer(channel, hep_buffer.get_parent_info, num_events, jet_type, cut)\n",
    "    if num_particles >= 1:\n",
    "        jet_daughter = hep_buffer.load_data_buffer(channel, hep_buffer.get_daughter_info, num_events, num_particles, jet_type, cut)\n",
    "        return torch.cat((jet_parent, jet_daughter), dim=1)\n",
    "    else:\n",
    "        return jet_parent\n",
    "\n",
    "class JetDataset(Dataset):\n",
    "    def __init__(self, signal_events, background_events, norm):\n",
    "        x = torch.cat((signal_events ,background_events), dim=0)\n",
    "        if norm: x = self.get_norm(x)\n",
    "        y = torch.cat((torch.ones((len(signal_events)), 1), torch.zeros((len(background_events)), 1)), dim=0)\n",
    "        x.requires_grad = False\n",
    "        y.requires_grad = False\n",
    "        self.x, self.y = x, y\n",
    "    def __getitem__(self, index):\n",
    "        return self.x[index], self.y[index]\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    def get_norm(self, events):\n",
    "        parent_pt_eta_phi   = events[:, :3].reshape(-1, 3, 1)\n",
    "        daughter_pt_eta_phi = events[:, -3*data_config[\"num_particles\"]:].reshape(-1, 3, data_config[\"num_particles\"])\n",
    "        parent_pt, parent_eta, parent_phi = parent_pt_eta_phi.transpose(0, 1)\n",
    "        daughter_pt, daughter_eta, daughter_phi = daughter_pt_eta_phi.transpose(0, 1)\n",
    "        pt_ratio, delta_eta, delta_phi = daughter_pt/parent_pt, daughter_eta-parent_eta, daughter_phi-parent_phi\n",
    "        delta_r, cluster_radius = torch.sqrt(delta_eta**2 + delta_phi**2), 1\n",
    "        norm_pt  = (1/daughter_pt) * delta_r / cluster_radius\n",
    "        norm_eta = delta_eta / delta_r\n",
    "        norm_phi = delta_phi / delta_r\n",
    "        if not ((torch.abs(norm_pt) <= 1).all() and (torch.abs(norm_eta) <= 1).all() and  (torch.abs(norm_phi) <= 1).all()):\n",
    "            num_norm_pt  = torch.sum(torch.abs(norm_pt) > 1)\n",
    "            num_norm_eta = torch.sum(torch.abs(norm_eta) > 1)\n",
    "            num_norm_phi = torch.sum(torch.abs(norm_phi) > 1)\n",
    "            print(f\"Recieve value greater than 1 in torch.asin() : (num_pt={num_norm_pt}, num_eta={num_norm_eta}, num_phi={num_norm_phi})\")\n",
    "            if num_norm_pt > 0:\n",
    "                norm_pt[norm_pt > 1] = 1\n",
    "                norm_pt[norm_pt < -1] = -1\n",
    "            if num_norm_eta > 0:\n",
    "                norm_eta[norm_eta > 1] = 1\n",
    "                norm_eta[norm_eta < -1] = -1\n",
    "            if num_norm_phi > 0:\n",
    "                norm_phi[norm_phi > 1] = 1\n",
    "                norm_phi[norm_phi < -1] = -1\n",
    "        events = torch.cat((events, norm_pt, norm_eta, norm_phi), dim=-1)\n",
    "        return events\n",
    "\n",
    "class JetDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, norm):\n",
    "        super().__init__()\n",
    "        self.norm = norm\n",
    "        # get signal and background data\n",
    "        self.signal_events     = get_data(data_config[\"signal_channel\"])\n",
    "        self.background_events = get_data(data_config[\"background_channel\"])\n",
    "        # determine num_train, num_valid, num_test\n",
    "        num_sig   = len(self.signal_events)\n",
    "        num_bkg   = len(self.background_events)\n",
    "        num_data  = min(num_sig, num_bkg, hyper_config[\"max_num_data\"])\n",
    "        num_train = int(hyper_config[\"train_ratio\"] * num_data)\n",
    "        num_valid = int(hyper_config[\"valid_ratio\"] * num_train)\n",
    "        num_test  = num_data - num_train\n",
    "        num_train = num_train - num_valid\n",
    "        self.num_train, self.num_valid, self.num_test = num_train, num_valid, num_test\n",
    "        print(f\"JetDataModule INFO: num_train = {num_train}, num_valid = {num_valid}, num_test = {num_test}\")\n",
    "\n",
    "    def setup(self, stage):\n",
    "        train_idx = self.num_train\n",
    "        valid_idx = self.num_train + self.num_valid\n",
    "        test_idx  = self.num_train + self.num_valid + self.num_test\n",
    "        if stage == \"fit\":\n",
    "            self.train_dataset = JetDataset(self.signal_events[:train_idx], self.background_events[:train_idx], norm=self.norm)\n",
    "            self.valid_dataset = JetDataset(self.signal_events[train_idx:valid_idx], self.background_events[train_idx:valid_idx], norm=self.norm)\n",
    "        elif stage == \"test\":\n",
    "            self.test_dataset  = JetDataset(self.signal_events[valid_idx:test_idx], self.background_events[valid_idx:test_idx], norm=self.norm)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=hyper_config[\"batch_size\"])\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.valid_dataset, batch_size=hyper_config[\"batch_size\"])\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_dataset, batch_size=hyper_config[\"batch_size\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classical Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassicalFNNModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, hidden_layers):\n",
    "        super().__init__()\n",
    "        if hidden_layers == 0:\n",
    "            net = [nn.Linear(input_dim, 1)]\n",
    "        else:\n",
    "            net = [nn.Linear(input_dim, hidden_dim), nn.ReLU()]\n",
    "            for _ in range(hidden_layers-1):\n",
    "                net += [nn.Linear(hidden_dim, hidden_dim)]\n",
    "                net += [nn.ReLU()]\n",
    "            net += [nn.Linear(hidden_dim, 1)]\n",
    "        # BCEWithLogitsLoss already contains a sigmoid function\n",
    "        self.net = nn.Sequential(*net)\n",
    "    def forward(self, x):\n",
    "        y = self.net(x)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantum Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Encoding layers (ENC) and Variational Quantum Circuit (VQC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ENCDaughterNorm:\n",
    "    def __init__(self, num_particles):\n",
    "        self.num_particles = num_particles\n",
    "        self.num_qubits = 3 * num_particles\n",
    "    def __call__(self, inputs):\n",
    "        inputs = inputs.reshape((3, -1))\n",
    "        norm_pt, norm_eta, norm_phi = inputs\n",
    "        for ptc in range(self.num_particles):\n",
    "            qml.RY(2 * torch.asin(norm_pt[ptc]), wires=3*ptc)\n",
    "            qml.RY(2 * torch.asin(norm_pt[ptc]), wires=3*ptc+1)\n",
    "            qml.CRY(2 * torch.asin(norm_eta[ptc]), wires=[3*ptc, 3*ptc+2])\n",
    "            qml.CRY(2 * torch.asin(norm_phi[ptc]), wires=[3*ptc+1, 3*ptc+2])\n",
    "\n",
    "class VQCRotCNOT:\n",
    "    def __init__(self, num_qubits, num_layers):\n",
    "        self.num_qubits = num_qubits\n",
    "        self.num_layers = num_layers\n",
    "    def __call__(self, weights):\n",
    "        num_qubits = self.num_qubits\n",
    "        num_layers = self.num_layers\n",
    "        for l in range(num_layers):\n",
    "            for q in range(num_qubits):\n",
    "                qml.Rot(*weights[l][q], wires=q)\n",
    "            for q in range(num_qubits):\n",
    "                if q != num_qubits-1:\n",
    "                    qml.CNOT(wires=[q, q+1])\n",
    "                else:\n",
    "                    if num_qubits >= 3:\n",
    "                        qml.CNOT(wires=[q, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Quantum Layers and Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qml_torch_layer(enc_layer, vqc_layer, weight_shapes, num_reupload):\n",
    "    num_qubits = max(enc_layer.num_qubits, vqc_layer.num_qubits)\n",
    "    dev = qml.device('default.qubit', wires=num_qubits)\n",
    "    @qml.qnode(dev)\n",
    "    def qnode(inputs, weights):\n",
    "        for r in range(num_reupload):\n",
    "            enc_layer(inputs)\n",
    "            vqc_layer(weights[r])\n",
    "        return [qml.expval(qml.PauliZ(wires=i)) for i in range(num_qubits)]\n",
    "    return qml.qnn.TorchLayer(qnode, weight_shapes)\n",
    "\n",
    "\n",
    "class HybridArcKernelDaughterModel(nn.Module):\n",
    "    def __init__(self, classical_model, num_particles, num_layers, num_reupload):\n",
    "        super().__init__()\n",
    "        self.num_particles = num_particles\n",
    "        num_qubits = 3 * num_particles\n",
    "        self.num_qubits = num_qubits\n",
    "        weight_shapes = {\"weights\":(num_reupload, num_layers, num_qubits, 3)}\n",
    "        enc_layer = ENCDaughterNorm(num_particles)\n",
    "        vqc_layer = VQCRotCNOT(num_qubits, num_layers)\n",
    "        self.quantum_kernel = qml_torch_layer(enc_layer, vqc_layer, weight_shapes, num_reupload)\n",
    "        self.classical_model = classical_model\n",
    "    def forward(self, x):\n",
    "        num_particles = self.num_particles\n",
    "        norm_pt_eta_phi = x[:, -3*num_particles:].reshape(-1, 3, num_particles)\n",
    "        norm_pt, norm_eta, norm_phi = norm_pt_eta_phi[:, 0], norm_pt_eta_phi[:, 1], norm_pt_eta_phi[:, 2]\n",
    "        quantum_input = torch.cat((norm_pt, norm_eta, norm_phi), dim=1)\n",
    "        x = torch.cat((x[:, :-3*num_particles], self.quantum_kernel(quantum_input)), dim=1)\n",
    "        y = self.classical_model(x)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LitModel(pl.LightningModule):\n",
    "    def __init__(self, model, *args, **kwargs):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.save_hyperparameters()\n",
    "        self.loss_func = hyper_config[\"loss_function\"]\n",
    "        self.acc_func = hyper_config[\"accuracy_function\"]\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y_true = batch\n",
    "        y_pred = self.model(x)\n",
    "        train_loss = self.loss_func(y_true, y_pred)\n",
    "        train_acc = self.acc_func(y_true, y_pred > 0.5)\n",
    "        self.log(\"train_loss\", train_loss, on_step=True, on_epoch=True)\n",
    "        self.log(\"train_acc\", train_acc, on_step=True, on_epoch=True)\n",
    "        return train_loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y_true = batch\n",
    "        y_pred = self.model(x)\n",
    "        valid_loss = self.loss_func(y_true, y_pred)\n",
    "        valid_acc = self.acc_func(y_true, y_pred > 0.5)\n",
    "        self.log(\"valid_loss\", valid_loss, on_step=True, on_epoch=True)\n",
    "        self.log(\"valid_acc\", valid_acc, on_step=True, on_epoch=True)\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y_true = batch\n",
    "        y_pred = self.model(x)\n",
    "        test_loss = self.loss_func(y_true, y_pred)\n",
    "        test_acc = self.acc_func(y_true, y_pred > 0.5)\n",
    "        self.log(\"test_loss\", test_loss, on_step=False, on_epoch=True)\n",
    "        self.log(\"test_acc\", test_acc, on_step=False, on_epoch=True)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=hyper_config[\"lr\"])\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data modules\n",
    "c_datamodule = JetDataModule(norm=False)\n",
    "q_datamodule = JetDataModule(norm=True)\n",
    "\n",
    "# classical\n",
    "input_dim = c_datamodule.signal_events.shape[1]\n",
    "hidden_dim, hidden_layers = 100 * input_dim, 2\n",
    "\n",
    "# quantum\n",
    "num_reupload = 1\n",
    "num_layers = 1\n",
    "\n",
    "c_nn = ClassicalFNNModel(input_dim, hidden_dim, hidden_layers)\n",
    "c_model = LitModel(c_nn)\n",
    "\n",
    "q_c_nn = ClassicalFNNModel(input_dim + 3*data_config[\"num_particles\"], hidden_dim, hidden_layers)\n",
    "q_nn = HybridArcKernelDaughterModel(q_c_nn, data_config[\"num_particles\"], num_layers, num_reupload)\n",
    "q_model = LitModel(q_nn)\n",
    "\n",
    "c_trainer = pl.Trainer(**trainer_config)\n",
    "c_trainer.fit(model=c_model, datamodule=c_datamodule)\n",
    "c_trainer.test(model=c_model, datamodule=c_datamodule)\n",
    "\n",
    "q_trainer = pl.Trainer(**trainer_config)\n",
    "q_trainer.fit(model=q_model, datamodule=q_datamodule)\n",
    "q_trainer.test(model=q_model, datamodule=q_datamodule)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit ('3.9.12')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "abdd0d95ca50f233d1202cce1ba28eab5ada50f7ec17823ef40ef9b79347f6f5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
