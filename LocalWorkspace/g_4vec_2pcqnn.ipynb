{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Packages\n",
    "\n",
    "- `awkward`: For dealing with nested, variable-sized data.\n",
    "- `pennylane`: Quantum machine learning.\n",
    "- `lightning`: Simplifying training process.\n",
    "- `pytorch_geometric`: Graph neural network package.\n",
    "- `wandb`: Monitoring training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic packages\n",
    "import os, time, random\n",
    "from itertools import product\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# data\n",
    "import awkward as ak\n",
    "import d_hep_data\n",
    "\n",
    "# qml\n",
    "import pennylane as qml\n",
    "from pennylane import numpy as np\n",
    "\n",
    "# pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# pytorch_lightning\n",
    "import lightning as L\n",
    "import lightning.pytorch as pl\n",
    "\n",
    "# pytorch_geometric\n",
    "import networkx as nx\n",
    "import torch_geometric.nn as geom_nn\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import MessagePassing\n",
    "\n",
    "# scipy\n",
    "from sklearn import metrics\n",
    "\n",
    "# wandb\n",
    "import wandb\n",
    "from lightning.pytorch.loggers import WandbLogger\n",
    "wandb.login()\n",
    "\n",
    "# reproducibility\n",
    "L.seed_everything(3020616)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# faster calculation on GPU but less precision\n",
    "torch.set_float32_matmul_precision(\"medium\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configurations\n",
    "\n",
    "Hyperparameters and configurations for:\n",
    "- Data (channel, .etc)\n",
    "- Training process (Trainer, .etc)\n",
    "- Model architecture (input/output dimension, .etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configuration dictionary\n",
    "cf = {}\n",
    "cf[\"time\"]     = time.strftime(\"%Y%m%d_%H%M%S\", time.localtime())\n",
    "cf[\"wandb\"]    = True\n",
    "cf[\"project\"]  = \"g_4vec_2pcqnn\"\n",
    "cf[\"rnd_seed\"] = None # to be determined by for loop\n",
    "\n",
    "# data infotmation\n",
    "cf[\"num_events\"]    = \"50000\"\n",
    "cf[\"sig_channel\"]   = \"ZprimeToZhToZinvhbb\"\n",
    "cf[\"bkg_channel\"]   = \"QCD_HT2000toInf\"\n",
    "cf[\"jet_type\"]      = \"fatjet\"\n",
    "cf[\"subjet_radius\"] = None # to be determined from [0.25, 0.5, 0.75]\n",
    "cf[\"cut_limit\"]     = (500, 1500)\n",
    "cf[\"bin\"]           = 10\n",
    "cf[\"num_bin_data\"]  = None # to be determined from [100, 200, 300]\n",
    "\n",
    "# traning configuration\n",
    "cf[\"num_train_ratio\"]   = 0.5\n",
    "cf[\"num_test_ratio\"]    = 0.5\n",
    "cf[\"batch_size\"]        = 64\n",
    "cf[\"num_workers\"]       = 0\n",
    "cf[\"max_epochs\"]        = 100\n",
    "cf[\"accelerator\"]       = \"cpu\"\n",
    "cf[\"fast_dev_run\"]      = False\n",
    "cf[\"log_every_n_steps\"] = cf[\"batch_size\"] // 2\n",
    "\n",
    "# model hyperparameters\n",
    "cf[\"loss_function\"]  = nn.BCEWithLogitsLoss()\n",
    "cf[\"optimizer\"]      = optim.Adam\n",
    "cf[\"learning_rate\"]  = 1E-3\n",
    "\n",
    "# 2PCNN hyperparameters\n",
    "cf[\"gnn_layers\"] = None # to be determined by grid search\n",
    "cf[\"mlp_layers\"] = None # to be determined by grid search"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Module"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, we train with data containing only the four momentum of particles. In order to reduce the size of the data (due to the long training time for quantum machine learning), we reduce the size of data by `fastjet` package by clustering particles again by `anti-kt algorithm` with smaller radius.\n",
    "\n",
    "The detail (source code) for creating fastjet reclustering events is in the `d_hep_data` file.\n",
    "\n",
    "To test the power of QML for learning space structure of data (geometric angles, e.g. $p_t$, $\\eta$, $\\phi$), we will use four momentum only (or z-boosted invariant variables $p_t$, $\\eta$, $\\phi$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def events_pt_eta_phi(events, norm):\n",
    "    if norm:\n",
    "        f1 = np.arctan(events[\"_pt\"] / events[\"pt\"])\n",
    "        f2 = np.pi / 2 * events[\"_delta_eta\"]\n",
    "        f3 = np.pi * events[\"_delta_phi\"]\n",
    "        arrays = ak.zip([f1, f2, f3])\n",
    "    else:\n",
    "        f1 = events[\"_pt\"]\n",
    "        f2 = events[\"_delta_eta\"]\n",
    "        f3 = events[\"_delta_phi\"]\n",
    "        arrays = ak.zip([f1, f2, f3])\n",
    "    arrays = arrays.to_list()\n",
    "    x = [torch.tensor(arrays[i], dtype=torch.float32) for i in range(len(arrays))]\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JetDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, sig_buffer, bkg_buffer, events_func, **kwargs):\n",
    "        '''Add a \"_\" prefix if it is a fastjet feature'''\n",
    "        super().__init__()\n",
    "        # jet events\n",
    "        sig_events = sig_buffer.get_uniform_bin_data()\n",
    "        bkg_events = bkg_buffer.get_uniform_bin_data()\n",
    "        sig_events = events_func(sig_events, **kwargs)\n",
    "        bkg_events = events_func(bkg_events, **kwargs)\n",
    "        self.sig_data_list = self._create_data_list(sig_events, 1)\n",
    "        self.bkg_data_list = self._create_data_list(bkg_events, 0)\n",
    "\n",
    "        # count the number of training, and testing\n",
    "        num_data = cf[\"bin\"] * cf[\"num_bin_data\"]\n",
    "        assert len(self.sig_data_list) >= num_data, f\"sig data not enough: {len(self.sig_data_list)} < {num_data}\"\n",
    "        assert len(self.bkg_data_list) >= num_data, f\"bkg data not enough: {len(self.bkg_data_list)} < {num_data}\"\n",
    "        num_train = int(num_data * cf[\"num_train_ratio\"])\n",
    "        num_test  = int(num_data * cf[\"num_test_ratio\"])\n",
    "        print(f\"DataLog: {cf['sig_channel']} has {len(self.sig_data_list)} events and {cf['bkg_channel']} has {len(self.bkg_data_list)} events.\")\n",
    "        print(f\"Choose num_data for each channel to be {num_data} | Each channel  has num_train = {num_train}, num_test = {num_test}\")\n",
    "\n",
    "        # prepare dataset for dataloader\n",
    "        train_idx = num_train\n",
    "        test_idx  = num_train + num_test\n",
    "        self.train_dataset = self.sig_data_list[:train_idx] + self.bkg_data_list[:train_idx]\n",
    "        self.test_dataset  = self.sig_data_list[train_idx:test_idx] + self.bkg_data_list[train_idx:test_idx]\n",
    "    \n",
    "    def _create_data_list(self, events, y):\n",
    "        # create pytorch_geometric \"Data\" object\n",
    "        data_list = []\n",
    "        for i in range(len(events)):\n",
    "            x = events[i]\n",
    "            edge_index = list(product(range(len(x)), range(len(x))))\n",
    "            edge_index = torch.tensor(edge_index).transpose(0, 1)\n",
    "            x.requires_grad, edge_index.requires_grad = False, False\n",
    "            data_list.append(Data(x=x, edge_index=edge_index, y=y))\n",
    "        random.shuffle(data_list)\n",
    "        return data_list\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=cf[\"batch_size\"], num_workers=cf[\"num_workers\"],  shuffle=True)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_dataset, batch_size=cf[\"batch_size\"], num_workers=cf[\"num_workers\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compare classical GNN with quantum GNN, we use `GraphConv` and `MessagePassing` with `pennylane` for classical and quantum repectively.\n",
    "\n",
    "- Why using `nn.ModuleList` instead of `nn.Sequential`?\n",
    "Both `nn.ModuleList` and `nn.Sequential` trace the trainable parameters autometically. However, since we are using \"gnn\" layers, we need to feed into additional argument `edge_index`. In order to check whether we are using \"gnn\" layers or not, we use `isinstance` to check the class type (Since all PyTorch Geometric graph layer inherit the class `MessagePassing`). For detail, see [When should I use nn.ModuleList and when should I use nn.Sequential?](https://discuss.pytorch.org/t/when-should-i-use-nn-modulelist-and-when-should-i-use-nn-sequential/5463/3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassicalMLP(nn.Module):\n",
    "    def __init__(self, in_channel, out_channel, hidden_channel, num_layers):\n",
    "        super().__init__()\n",
    "        if num_layers == 0:\n",
    "            self.net = nn.Linear(in_channel, out_channel)\n",
    "        else:\n",
    "            net = [nn.Linear(in_channel, hidden_channel), nn.ReLU()]\n",
    "            for _ in range(num_layers-2):\n",
    "                net += [nn.Linear(hidden_channel, hidden_channel), nn.ReLU()]\n",
    "            net += [nn.Linear(hidden_channel, out_channel)]\n",
    "            self.net = nn.Sequential(*net)\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "    \n",
    "class QuantumMLP(nn.Module):\n",
    "    def __init__(self, num_qubits, num_layers, num_reupload, measurements):\n",
    "        super().__init__()\n",
    "        # create a quantum MLP\n",
    "        @qml.qnode(qml.device('lightning.qubit', wires=num_qubits), diff_method=\"adjoint\")\n",
    "        def circuit(inputs, weights):\n",
    "            for i in range(num_reupload):\n",
    "                qml.AngleEmbedding(features=inputs, wires=range(num_qubits), rotation='Y')\n",
    "                qml.StronglyEntanglingLayers(weights=weights[i], wires=range(num_qubits))\n",
    "            measurements_dict = {\"X\":qml.PauliX, \"Y\":qml.PauliY, \"Z\":qml.PauliZ}\n",
    "            return [qml.expval(measurements_dict[m[1]](wires=m[0])) for m in measurements]\n",
    "        # turn the quantum circuit into a torch layer\n",
    "        weight_shapes = {\"weights\":(num_reupload, num_layers, num_qubits, 3)}\n",
    "        net = [qml.qnn.TorchLayer(circuit, weight_shapes=weight_shapes)]\n",
    "        self.net = nn.Sequential(*net)\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classical 2PCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classical2PCNNForwardMP(MessagePassing):\n",
    "    def __init__(self, num_features, num_layers, aggr):\n",
    "        super().__init__(aggr=aggr)\n",
    "        self.mp_phi = ClassicalMLP(\n",
    "            in_channel     = 2*num_features,\n",
    "            out_channel    = num_features,\n",
    "            hidden_channel = 2*num_features,\n",
    "            num_layers     = num_layers,\n",
    "            )\n",
    "        self.mp_gamma = ClassicalMLP(\n",
    "            in_channel     = 2*num_features, \n",
    "            out_channel    = num_features,\n",
    "            hidden_channel = 2*num_features,\n",
    "            num_layers     = num_layers,\n",
    "        )\n",
    "    def forward(self, x, edge_index):\n",
    "        return self.propagate(edge_index, x=x)\n",
    "    def message(self, x_i, x_j):\n",
    "        return self.mp_phi(torch.cat((x_i, x_j), dim=-1))\n",
    "    def update(self, aggr_out, x):\n",
    "        return self.mp_gamma(torch.cat((x, aggr_out), dim=-1))\n",
    "\n",
    "class Classical2PCNNForward(nn.Module):\n",
    "    def __init__(self, gnn_in, gnn_layers, gnn_aggr, mlp_in, mlp_out, mlp_hidden, mlp_layers):\n",
    "        super().__init__()\n",
    "        self.gnn = Classical2PCNNForwardMP(gnn_in, gnn_layers, gnn_aggr)\n",
    "        self.mlp = ClassicalMLP(mlp_in, mlp_out, mlp_hidden, mlp_layers)\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        x = self.gnn(x, edge_index)\n",
    "        x = geom_nn.global_mean_pool(x, batch)\n",
    "        x = self.mlp(x)\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantum 2PCQNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Quantum2PCQNNForwardMP(MessagePassing):\n",
    "    def __init__(self, num_features, num_layers, num_reupload, aggr):\n",
    "        super().__init__(aggr=aggr)\n",
    "        measurements = [[i, \"Z\"] for i in range(num_features)]\n",
    "        self.mp_phi = QuantumMLP(\n",
    "            num_qubits   = 2*num_features, \n",
    "            num_layers   = num_layers,\n",
    "            num_reupload = num_reupload,\n",
    "            measurements = measurements,\n",
    "            )\n",
    "        self.mp_gamma = ClassicalMLP(\n",
    "            in_channel     = 2*len(measurements), \n",
    "            out_channel    = num_features,\n",
    "            hidden_channel = 2*num_features,\n",
    "            num_layers     = num_layers,\n",
    "        )\n",
    "    def forward(self, x, edge_index):\n",
    "        return self.propagate(edge_index, x=x)\n",
    "    def message(self, x_i, x_j):\n",
    "        return self.mp_phi(torch.cat((x_i, x_j), dim=-1))\n",
    "    def update(self, aggr_out, x):\n",
    "        return self.mp_gamma(torch.cat((x, aggr_out), dim=-1))\n",
    "\n",
    "class Quantum2PCQNNForward(nn.Module):\n",
    "    def __init__(self, gnn_in, gnn_layers, gnn_reupload, gnn_aggr, mlp_in, mlp_out, mlp_hidden, mlp_layers):\n",
    "        super().__init__()\n",
    "        self.gnn = Quantum2PCQNNForwardMP(gnn_in, gnn_layers, gnn_reupload, gnn_aggr)\n",
    "        self.mlp = ClassicalMLP(mlp_in, mlp_out, mlp_hidden, mlp_layers)\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        x = self.gnn(x, edge_index)\n",
    "        x = geom_nn.global_mean_pool(x, batch)\n",
    "        x = self.mlp(x)\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lightning Module\n",
    "\n",
    "Most of the hyperparameters are defined at `cf` configuration dictionary.\n",
    "\n",
    "Note that when using `nn.BCEWithLogitsLoss`, the first argument should not be paased to `sigmoid`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LitModel(L.LightningModule):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters(ignore=['model'])\n",
    "        self.model = model\n",
    "        self.loss_function = cf[\"loss_function\"]\n",
    "\n",
    "    def forward(self, data):\n",
    "        # predict y\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "        x = self.model(x, edge_index, batch)\n",
    "        x = x.squeeze(dim=-1)\n",
    "\n",
    "        # calculate loss and accuracy\n",
    "        y_pred = x > 0\n",
    "        y_true = data.y\n",
    "        loss   = self.loss_function(x, y_true.float())\n",
    "        acc    = (y_pred == data.y).float().mean()\n",
    "\n",
    "        # calculate auc\n",
    "        y_score = torch.sigmoid(x).detach()\n",
    "        self.y_true_buffer  = torch.cat((self.y_true_buffer, y_true))\n",
    "        self.y_score_buffer = torch.cat((self.y_score_buffer, y_score))\n",
    "        return loss, acc\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = cf[\"optimizer\"](self.parameters(), lr=cf[\"learning_rate\"])\n",
    "        return optimizer\n",
    "\n",
    "    def on_train_epoch_start(self):\n",
    "        self.start_time     = time.time()\n",
    "        self.y_true_buffer  = torch.tensor([])\n",
    "        self.y_score_buffer = torch.tensor([])\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        self.end_time = time.time()\n",
    "        delta_time = self.end_time - self.start_time\n",
    "        roc_auc    = metrics.roc_auc_score(self.y_true_buffer, self.y_score_buffer)\n",
    "        self.log(\"epoch_time\", delta_time, on_step=False, on_epoch=True)\n",
    "        self.log(\"train_roc_auc\", roc_auc, on_step=False, on_epoch=True)\n",
    "\n",
    "    def on_test_epoch_start(self):\n",
    "        self.y_true_buffer  = torch.tensor([])\n",
    "        self.y_score_buffer = torch.tensor([])\n",
    "\n",
    "    def on_test_epoch_end(self):\n",
    "        roc_auc = metrics.roc_auc_score(self.y_true_buffer, self.y_score_buffer)\n",
    "        self.log(\"test_roc_auc\", roc_auc, on_step=False, on_epoch=True)\n",
    "\n",
    "    def training_step(self, data, batch_idx):\n",
    "        loss, acc = self.forward(data)\n",
    "        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, batch_size=len(data.x))\n",
    "        self.log(\"train_acc\", acc, on_step=True, on_epoch=True, batch_size=len(data.x))\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, data, batch_idx):\n",
    "        _, acc = self.forward(data)\n",
    "        self.log(\"test_acc\", acc, on_step=True, on_epoch=True, batch_size=len(data.x))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Test the Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data_module, commit=\"\", suffix=\"\"):\n",
    "    # setup id and path for saving\n",
    "    project  = cf['project']\n",
    "    group    = f\"{cf['time']}_{commit}_{cf['sig_channel']}_{cf['bkg_channel']}_{cf['jet_type']}\"\n",
    "    job_type = f\"R{cf['subjet_radius']}_B{cf['bin']}_D{cf['num_bin_data']}\"\n",
    "    name     = f\"{model.__class__.__name__}_gl{cf['gnn_layers']}_ml{cf['mlp_layers']}_{suffix} | {job_type} | rnd_{cf['rnd_seed']} | {cf['time']}\"\n",
    "    id       = f\"{name}\"\n",
    "    tags     = [model.__class__.__name__, cf['sig_channel'], cf['bkg_channel'], cf['jet_type'], str(cf['subjet_radius'])]\n",
    "    root_dir = f\"./result\"\n",
    "    if not os.path.isdir(root_dir):\n",
    "        os.makedirs(root_dir)\n",
    "\n",
    "    # check whether wandb name exists\n",
    "    api     = wandb.Api()\n",
    "    project = cf[\"project\"]\n",
    "    if id in set([run.id for run in api.runs(f\"ntuyianchen/{project}\")]):\n",
    "        print(f\"{id} already exists, skip this run\")\n",
    "        return\n",
    "\n",
    "    # wandb logger setup\n",
    "    if cf[\"wandb\"]:\n",
    "        cf[\"model_structure\"] = f\"gl{cf['gnn_layers']}_ml{cf['mlp_layers']}\"\n",
    "        cf[\"model_name\"]      = model.__class__.__name__\n",
    "        cf[\"group_rnd_seed\"]  = f\"{cf['model_name']}_{cf['model_structure']}_{suffix} | {job_type}\"\n",
    "        cf[\"suffix\"]          = suffix\n",
    "        wandb_logger = WandbLogger(project=project, group=group, job_type=job_type, name=name, id=id, save_dir=root_dir, tags=tags)\n",
    "        wandb_logger.experiment.config.update(cf)\n",
    "        wandb_logger.watch(model, log=\"all\")\n",
    "\n",
    "    # start lightning training\n",
    "    logger   = wandb_logger if cf[\"wandb\"] else None\n",
    "    trainer  = L.Trainer(\n",
    "        logger=logger, \n",
    "        accelerator       = cf[\"accelerator\"],\n",
    "        max_epochs        = cf[\"max_epochs\"],\n",
    "        fast_dev_run      = cf[\"fast_dev_run\"],\n",
    "        log_every_n_steps = cf[\"log_every_n_steps\"],\n",
    "        )\n",
    "    litmodel = LitModel(model)\n",
    "    trainer.fit(litmodel, datamodule=data_module)\n",
    "    trainer.test(litmodel, datamodule=data_module)\n",
    "\n",
    "    # finish wandb monitoring\n",
    "    if cf[\"wandb\"]:\n",
    "        wandb.finish()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start training each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_train(sig_buffer, bkg_buffer, commit=\"\"):\n",
    "    # setup\n",
    "    L.seed_everything(cf[\"rnd_seed\"])\n",
    "\n",
    "    # data module\n",
    "    data_pt_eta_phi = JetDataModule(sig_buffer, bkg_buffer, events_func=events_pt_eta_phi, norm=False)\n",
    "    data_pt_eta_phi_norm = JetDataModule(sig_buffer, bkg_buffer, events_func=events_pt_eta_phi, norm=True)\n",
    "\n",
    "    # classical 2pcnn\n",
    "    input_dim = data_pt_eta_phi.train_dataset[0].x.shape[1]\n",
    "    cf_2pcnn = {\n",
    "        \"gnn_in\":input_dim, \n",
    "        \"gnn_layers\":cf[\"gnn_layers\"],\n",
    "        \"gnn_aggr\":\"add\", \n",
    "        \"mlp_in\":input_dim,\n",
    "        \"mlp_out\":1, \n",
    "        \"mlp_hidden\":3*input_dim, \n",
    "        \"mlp_layers\":cf[\"mlp_layers\"],\n",
    "    }\n",
    "    train(Classical2PCNNForward(**cf_2pcnn), data_pt_eta_phi, commit, suffix=f\"pep\")\n",
    "    train(Classical2PCNNForward(**cf_2pcnn), data_pt_eta_phi_norm, commit, suffix=f\"pep_norm\")\n",
    "\n",
    "    # quantum 2pcqnn\n",
    "    input_dim = data_pt_eta_phi_norm.train_dataset[0].x.shape[1]\n",
    "    cf_2pcqnn = {\n",
    "        \"gnn_in\":input_dim,\n",
    "        \"gnn_layers\":cf[\"gnn_layers\"],\n",
    "        \"gnn_reupload\":input_dim,\n",
    "        \"gnn_aggr\":\"add\",\n",
    "        \"mlp_in\":input_dim,\n",
    "        \"mlp_out\":1,\n",
    "        \"mlp_hidden\":3*input_dim,\n",
    "        \"mlp_layers\":cf[\"mlp_layers\"],\n",
    "    }\n",
    "    train(Quantum2PCQNNForward(**cf_2pcqnn), data_pt_eta_phi_norm, commit, suffix=f\"pep_norm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use old date to keep run break runs\n",
    "old_time = input(\"Input old time if needed else just press enter: \")\n",
    "if old_time != \"\" and old_time != None:\n",
    "    cf[\"time\"] = old_time\n",
    "\n",
    "# short description of this experiment\n",
    "commit = input(\"Commit of this experiment (short description): \")\n",
    "\n",
    "print(f\"Date = {cf['time']} | Commit = {commit}\")\n",
    "input(\"Press enter to continue\")\n",
    "for subjet_radius in [0.75, 0.5, 0.25]:\n",
    "    for num_bin_data in [50, 100]:\n",
    "        cf[\"subjet_radius\"] = subjet_radius\n",
    "        cf[\"num_bin_data\"]  = num_bin_data\n",
    "        arg_buffer = [cf[\"num_events\"], cf[\"jet_type\"], cf[\"subjet_radius\"], cf[\"cut_limit\"], cf[\"bin\"], cf[\"num_bin_data\"]]\n",
    "        sig_buffer = d_hep_data.UniformBinJetBuffer(cf[\"sig_channel\"], *arg_buffer)\n",
    "        bkg_buffer = d_hep_data.UniformBinJetBuffer(cf[\"bkg_channel\"], *arg_buffer)\n",
    "        for gl, ml in [(1,2), (1,4)]:\n",
    "            cf[\"gnn_layers\"] = gl\n",
    "            cf[\"mlp_layers\"] = ml\n",
    "            for rnd_seed in range(5):\n",
    "                cf[\"rnd_seed\"] = rnd_seed\n",
    "                grid_train(sig_buffer, bkg_buffer, commit)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "abdd0d95ca50f233d1202cce1ba28eab5ada50f7ec17823ef40ef9b79347f6f5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
