{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CMS Jet Data Machine Learning with Arc Kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import jet\n",
    "import uproot\n",
    "import awkward as ak\n",
    "\n",
    "import pennylane as qml\n",
    "import pennylane.numpy as np\n",
    "\n",
    "import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_theme()\n",
    "\n",
    "def set_rnd_seed():\n",
    "    np.random.seed(0)\n",
    "    torch.manual_seed(0)\n",
    "    torch.cuda.manual_seed(0)\n",
    "\n",
    "GPU = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Classical layers and models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassicalModel(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, hidden_layers):\n",
    "        super().__init__()\n",
    "        if hidden_layers == 0:\n",
    "            net = [nn.Linear(input_dim, 1)]\n",
    "        else:\n",
    "            net = [nn.Linear(input_dim, hidden_dim), nn.ReLU()]\n",
    "            for _ in range(hidden_layers-1):\n",
    "                net += [nn.Linear(hidden_dim, hidden_dim)]\n",
    "                net += [nn.ReLU()]\n",
    "            net += [nn.Linear(hidden_dim, 1)]\n",
    "        # BCEWithLogitsLoss already contains a sigmoid function\n",
    "        self.net = nn.Sequential(*net)\n",
    "    def forward(self, x):\n",
    "        y = self.net(x)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Quantum layers and models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_daughter_pt_ratio_delta(inputs, cluster_r=1):\n",
    "    inputs = inputs.reshape((3, -1))\n",
    "    num_particles = inputs.shape[1]\n",
    "    num_qubits = 3 * num_particles\n",
    "    norm_pt, norm_eta, norm_phi = inputs\n",
    "    for ptc in range(num_particles):\n",
    "        qml.RY(2 * torch.asin(norm_pt[ptc]), wires=3*ptc)\n",
    "        qml.RY(2 * torch.asin(norm_pt[ptc]), wires=3*ptc+1)\n",
    "        qml.CRY(2 * torch.asin(norm_eta[ptc]), wires=[3*ptc, 3*ptc+2])\n",
    "        qml.CRY(2 * torch.asin(norm_phi[ptc]), wires=[3*ptc+1, 3*ptc+2])\n",
    "\n",
    "def vqc_rot_cnot(num_qubits, weights, num_layers):\n",
    "    for l in range(num_layers):\n",
    "        for q in range(num_qubits):\n",
    "            qml.Rot(*weights[l][q], wires=q)\n",
    "        for q in range(num_qubits):\n",
    "            if q != num_qubits-1:\n",
    "                qml.CNOT(wires=[q, q+1])\n",
    "            else:\n",
    "                if num_qubits >= 3:\n",
    "                    qml.CNOT(wires=[q, 0])\n",
    "\n",
    "def qml_torch_layer(num_qubits, weight_shapes, enc_layer, qml_layer, num_reupload):\n",
    "    dev = qml.device('default.qubit', wires=num_qubits)\n",
    "    @qml.qnode(dev)\n",
    "    def qnode(inputs, weights):\n",
    "        for r in range(num_reupload):\n",
    "            enc_layer(inputs)\n",
    "            qml_layer(num_qubits, weights[r])\n",
    "        return [qml.expval(qml.PauliZ(wires=i)) for i in range(num_qubits)]\n",
    "    return qml.qnn.TorchLayer(qnode, weight_shapes)\n",
    "\n",
    "class HybridModel(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, hidden_layers, num_qubits, weight_shapes, enc_layer, qml_layer, num_reupload):\n",
    "        super().__init__()\n",
    "        self.num_particles = num_qubits // 3\n",
    "        self.qkernel = qml_torch_layer(num_qubits, weight_shapes, enc_layer, qml_layer, num_reupload)\n",
    "        self.net = ClassicalModel(input_dim+num_qubits, hidden_dim, hidden_layers)\n",
    "    def forward(self, x):\n",
    "        num_particles = self.num_particles\n",
    "        norm_pt_eta_phi = x[:, -3*num_particles:].reshape(-1, 3, num_particles)\n",
    "        norm_pt, norm_eta, norm_phi = norm_pt_eta_phi[:, 0], norm_pt_eta_phi[:, 1], norm_pt_eta_phi[:, 2]\n",
    "        circuit_input = torch.cat((norm_pt, norm_eta, norm_phi), dim=1)\n",
    "        x = torch.cat((x[:, :-3*num_particles], self.qkernel(circuit_input)), dim=1)\n",
    "        y = self.net(x)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data_loader, config):\n",
    "    print(f\"Training with device : {GPU}\")\n",
    "    model = model.to(GPU)\n",
    "    loss = nn.BCEWithLogitsLoss(reduction=\"mean\")\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=config[\"learning_rate\"], weight_decay=config[\"weight_decay\"])\n",
    "    record = {\"train_loss\":[], \"train_acc\":[], \"test_loss\":[], \"test_acc\":[]}\n",
    "    for epoch in range(config[\"num_epochs\"]):\n",
    "        train_loss, train_acc, test_loss, test_acc = 0, 0, 0, 0\n",
    "        model.train()\n",
    "        for x, y_true in tqdm.tqdm(data_loader[\"train\"], desc=f\"Train({len(data_loader['train'].dataset)})\"):\n",
    "            x, y_true = x.to(GPU), y_true.to(GPU)\n",
    "            opt.zero_grad()\n",
    "            y_pred = model(x)\n",
    "            batch_loss = loss(y_pred, y_true)\n",
    "            batch_loss.backward()\n",
    "            train_loss += batch_loss.detach().cpu()\n",
    "            # BCEWithLogitsLoss -> >=0 | Sigmoid + BCELoss -> >=0.5\n",
    "            train_acc += (torch.sum((y_pred >= 0) == y_true) / len(x))\n",
    "            opt.step()\n",
    "        model.eval()\n",
    "        for x, y_true in tqdm.tqdm(data_loader[\"test\"], desc=f\"Test({len(data_loader['test'].dataset)})\"):\n",
    "            x, y_true = x.to(GPU), y_true.to(GPU)\n",
    "            y_pred = model(x)\n",
    "            batch_loss = loss(y_pred, y_true)\n",
    "            test_loss += (batch_loss * len(x)).detach().cpu()\n",
    "            # BCEWithLogitsLoss -> >=0 | Sigmoid + BCELoss -> >=0.5\n",
    "            test_acc += (torch.sum((y_pred >= 0) == y_true).item())\n",
    "        train_loss /= len(data_loader[\"train\"])\n",
    "        train_acc /= len(data_loader[\"train\"])\n",
    "        test_loss /= len(data_loader[\"test\"].dataset)\n",
    "        test_acc /= len(data_loader[\"test\"].dataset)\n",
    "        record[\"train_loss\"].append(train_loss)\n",
    "        record[\"train_acc\"].append(train_acc)\n",
    "        record[\"test_loss\"].append(test_loss)\n",
    "        record[\"test_acc\"].append(test_acc)\n",
    "        print(f\"Epoch {epoch+1} : train = (loss:{train_loss:.2f}, acc:{train_acc:.2f}) | test = (loss:{test_loss:.2f}, acc:{test_acc:.2f})\")\n",
    "    return record"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_buffer(channel, get_method, *args):\n",
    "    suffix = \" \".join(map(str, args))\n",
    "    buffer_file = f\"data_buffer/{channel}-{get_method.__name__}-{suffix}.pt\"\n",
    "    if not os.path.exists(buffer_file):\n",
    "        print(f\"Buffer ({get_method.__name__}) : {channel}.pt not found, create now ...\")\n",
    "        events = get_method(channel, *args)\n",
    "        torch.save(events, buffer_file)\n",
    "    else:\n",
    "        events = torch.load(buffer_file)\n",
    "        print(f\"Buffer ({get_method.__name__}) : {channel}.pt found, loading complete!\")\n",
    "    return events\n",
    "\n",
    "def get_events(channel, num_events, num_particles, jet_type, cut):\n",
    "    jet_parent = load_data_buffer(channel, jet.get_parent_info, num_events, jet_type, cut)\n",
    "    if num_particles >= 1:\n",
    "        jet_daughter = load_data_buffer(channel, jet.get_daughter_info, num_events, num_particles, jet_type, cut)\n",
    "        return torch.cat((jet_parent, jet_daughter), dim=1)\n",
    "    else:\n",
    "        return jet_parent\n",
    "\n",
    "jet_type   = \"fatjet\"\n",
    "num_events = 50000\n",
    "num_particles = 3\n",
    "cut = f\"({jet_type}_pt >= 900) & ({jet_type}_pt <= 1100)\"\n",
    "\n",
    "signal_channel = \"ZprimeToZhToZinvhbb\"\n",
    "# signal_channel = \"ZprimeToZhToZlephbb\"\n",
    "# background_channel = \"QCD_HT1500to2000\"\n",
    "background_channel = \"QCD_HT2000toInf\"\n",
    "\n",
    "data_ratio = 0.9\n",
    "signal_events = get_events(signal_channel, num_events, num_particles, jet_type, cut)\n",
    "background_events = get_events(background_channel, num_events, num_particles, jet_type, cut)\n",
    "num_sig, num_bkg = len(signal_events), len(background_events)\n",
    "num_data = min(num_sig, num_bkg, 2000)\n",
    "num_train = int(data_ratio * num_data)\n",
    "num_test = num_data - num_train\n",
    "print(\"-\" * 100)\n",
    "print(f\"Signal = {signal_channel} | Background = {background_channel} | Cut = {cut}\")\n",
    "print(f\"Length Signal = {num_sig} | Length Background = {num_bkg} | Number of Data = {num_data} | Shape = {signal_events.shape}\")\n",
    "print(f\"number of training data = {num_train} | number of testing data = {num_test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JetDataset(Dataset):\n",
    "    def __init__(self, signal_events, background_events, num_particles, norm):\n",
    "        x = torch.cat((signal_events ,background_events), dim=0)\n",
    "        y = torch.cat((torch.ones((len(signal_events)), 1), torch.zeros((len(background_events)), 1)), dim=0)\n",
    "        # add norm_pt, norm_eta, norm_phi\n",
    "        if num_particles > 0 and norm == True:\n",
    "            parent_pt_eta_phi = x[:, :3].reshape(-1, 3, 1)\n",
    "            parent_pt, parent_eta, parent_phi = parent_pt_eta_phi[:, 0], parent_pt_eta_phi[:, 1], parent_pt_eta_phi[:, 2]\n",
    "            daughter_pt_eta_phi = x[:, -num_particles*3:].reshape(-1, 3, num_particles)\n",
    "            daughter_pt, daughter_eta, daughter_phi = daughter_pt_eta_phi[:, 0], daughter_pt_eta_phi[:, 1], daughter_pt_eta_phi[:, 2]\n",
    "            pt_ratio, delta_eta, delta_phi = daughter_pt/parent_pt, daughter_eta-parent_eta, daughter_phi-parent_phi\n",
    "            delta_r, cluster_radius = torch.sqrt(delta_eta**2 + delta_phi**2), 1\n",
    "            norm_pt  = pt_ratio * delta_r / cluster_radius\n",
    "            norm_eta = delta_eta / delta_r\n",
    "            norm_phi = delta_phi / delta_r\n",
    "            if not ((torch.abs(norm_pt) <= 1).all() and (torch.abs(norm_eta) <= 1).all() and  (torch.abs(norm_phi) <= 1).all()):\n",
    "                num_norm_pt  = torch.sum(torch.abs(norm_pt) > 1)\n",
    "                num_norm_eta = torch.sum(torch.abs(norm_eta) > 1)\n",
    "                num_norm_phi = torch.sum(torch.abs(norm_phi) > 1)\n",
    "                print(f\"Recieve value greater than 1 in torch.asin() : (pt={num_norm_pt}, eta={num_norm_eta}, phi={num_norm_phi})\")\n",
    "                if num_norm_pt > 0:\n",
    "                    norm_pt[norm_pt > 1] = 1\n",
    "                    norm_pt[norm_pt < -1] = -1\n",
    "                if num_norm_eta > 0:\n",
    "                    norm_eta[norm_eta > 1] = 1\n",
    "                    norm_eta[norm_eta < -1] = -1\n",
    "                if num_norm_phi > 0:\n",
    "                    norm_phi[norm_phi > 1] = 1\n",
    "                    norm_phi[norm_phi < -1] = -1\n",
    "            x = torch.cat((x, norm_pt, norm_eta, norm_phi), dim=-1)\n",
    "        x.requires_grad = False\n",
    "        y.requires_grad = False\n",
    "        self.x, self.y = x, y\n",
    "    def __getitem__(self, index):\n",
    "        return self.x[index], self.y[index]\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "c_data_train = JetDataset(signal_events[:num_train], background_events[:num_train], num_particles, norm=False)\n",
    "c_data_test = JetDataset(signal_events[num_train:num_data], background_events[num_train:num_data], num_particles, norm=False)\n",
    "q_data_train = JetDataset(signal_events[:num_train], background_events[:num_train], num_particles, norm=True)\n",
    "q_data_test = JetDataset(signal_events[num_train:num_data], background_events[num_train:num_data], num_particles, norm=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf = {\n",
    "    \"learning_rate\":1E-3,\n",
    "    \"weight_decay\":0,\n",
    "    \"num_epochs\":30,\n",
    "    \"batch_size\":32,\n",
    "}\n",
    "\n",
    "c_data_loader = {\n",
    "    \"train\":DataLoader(c_data_train, cf[\"batch_size\"], shuffle=True, drop_last=True),\n",
    "    \"test\":DataLoader(c_data_test, cf[\"batch_size\"], shuffle=False, drop_last=False),\n",
    "    }\n",
    "q_data_loader = {\n",
    "    \"train\":DataLoader(q_data_train, cf[\"batch_size\"], shuffle=True, drop_last=True),\n",
    "    \"test\":DataLoader(q_data_test, cf[\"batch_size\"], shuffle=False, drop_last=False),\n",
    "    }\n",
    "\n",
    "def grid_train(num_reupload):\n",
    "    set_rnd_seed()\n",
    "    # classical config\n",
    "    input_dim = signal_events.shape[1]\n",
    "    hidden_dim, hidden_layers = 100 * input_dim, 2\n",
    "\n",
    "    # quantum config\n",
    "    num_qubits = 3 * num_particles\n",
    "    num_qlayers = 1\n",
    "    # num_reupload = 2\n",
    "    weight_shapes = {\"weights\" : (num_reupload, num_qlayers, num_qubits, 3)}\n",
    "    enc_layer = encode_daughter_pt_ratio_delta\n",
    "    qml_layer = lambda x, y: vqc_rot_cnot(x, y, num_qlayers)\n",
    "\n",
    "    suffix = f\"{jet_type}_{signal_channel}_vs_{background_channel}_{''.join(cut.split())}_\"\n",
    "    suffix += f\"stc_id{input_dim}_hd{hidden_dim}_hl{hidden_layers}_rl{num_reupload}_\"\n",
    "    suffix += f\"qw_{weight_shapes['weights']}_np{num_particles}_nq{num_qubits}_nl{num_qlayers}\"\n",
    "\n",
    "    c_train_mode = True\n",
    "    q_train_mode = True\n",
    "    if c_train_mode:\n",
    "        c_model = ClassicalModel(input_dim, hidden_dim, hidden_layers)\n",
    "        c_result = train(c_model, c_data_loader, cf)\n",
    "        np.save(f\"result_arckernel/c_{suffix}.npy\", c_result)\n",
    "    if q_train_mode:\n",
    "        q_model = HybridModel(input_dim, hidden_dim, hidden_layers, num_qubits, weight_shapes, enc_layer, qml_layer, num_reupload)\n",
    "        q_result = train(q_model, q_data_loader, cf)\n",
    "        np.save(f\"result_arckernel/q_{enc_layer.__name__}_{suffix}.npy\", q_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for num_reupload in [1,2,3,4]:\n",
    "    grid_train(num_reupload)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit ('shims')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6f02055337d253f388db8c747d2eda41151d1a914a22500b5c34c6d448cd4f3e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
