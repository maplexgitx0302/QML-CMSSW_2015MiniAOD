{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Packages\n",
    "\n",
    "- `awkward`: For dealing with nested, variable-sized data.\n",
    "- `pennylane`: Quantum machine learning.\n",
    "- `lightning`: Simplifying training process.\n",
    "- `pytorch_geometric`: Graph neural network package.\n",
    "- `wandb`: Monitoring training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#--------------------------------------------------------------------------\n",
      "#                         FastJet release 3.4.0\n",
      "#                 M. Cacciari, G.P. Salam and G. Soyez                  \n",
      "#     A software package for jet finding and analysis at colliders      \n",
      "#                           http://fastjet.fr                           \n",
      "#\t                                                                      \n",
      "# Please cite EPJC72(2012)1896 [arXiv:1111.6097] if you use this package\n",
      "# for scientific work and optionally PLB641(2006)57 [hep-ph/0512210].   \n",
      "#                                                                       \n",
      "# FastJet is provided without warranty under the GNU GPL v2 or higher.  \n",
      "# It uses T. Chan's closest pair algorithm, S. Fortune's Voronoi code,\n",
      "# CGAL and 3rd party plugin jet algorithms. See COPYING file for details.\n",
      "#--------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yianchen/.pyenv/versions/3.9.12/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "ERROR:wandb.jupyter:Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mntuyianchen\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "INFO: Global seed set to 3020616\n",
      "INFO:lightning.fabric.utilities.seed:Global seed set to 3020616\n"
     ]
    }
   ],
   "source": [
    "# basic packages\n",
    "import os, time, random\n",
    "from itertools import product\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# data\n",
    "import awkward as ak\n",
    "from d_hep_data import JetEvents\n",
    "\n",
    "# qml\n",
    "import pennylane as qml\n",
    "from pennylane import numpy as np\n",
    "\n",
    "# pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# pytorch_lightning\n",
    "import lightning as L\n",
    "import lightning.pytorch as pl\n",
    "from lightning.pytorch.tuner import Tuner\n",
    "\n",
    "# pytorch_geometric\n",
    "import networkx as nx\n",
    "import torch_geometric.nn as geom_nn\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import MessagePassing\n",
    "\n",
    "# wandb\n",
    "import wandb\n",
    "from lightning.pytorch.loggers import WandbLogger\n",
    "wandb.login()\n",
    "\n",
    "# reproducibility\n",
    "L.seed_everything(3020616)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# faster calculation on GPU but less precision\n",
    "torch.set_float32_matmul_precision(\"medium\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configurations\n",
    "\n",
    "Hyperparameters and configurations for:\n",
    "- Data (channel, .etc)\n",
    "- Training process (Trainer, .etc)\n",
    "- Model architecture (input/output dimension, .etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configuration dictionary\n",
    "cf = {}\n",
    "cf[\"time\"]    = time.strftime(\"%Y%m%d_%H%M%S\", time.localtime())\n",
    "cf[\"wandb\"]   = True\n",
    "cf[\"project\"] = \"t_4vec_2pcnn\"\n",
    "\n",
    "# data infotmation\n",
    "cf[\"num_events\"]    = \"5000\"\n",
    "cf[\"sig_channel\"]   = \"ZprimeToZhToZinvhbb\"\n",
    "cf[\"bkg_channel\"]   = \"QCD_HT2000toInf\"\n",
    "cf[\"jet_type\"]      = \"fatjet\"\n",
    "cf[\"cut\"]           = f\"({cf['jet_type']}_pt>=500)&({cf['jet_type']}_pt<=1500)\"\n",
    "cf[\"subjet_radius\"] = 0.25\n",
    "\n",
    "# traning configuration\n",
    "cf[\"num_train_ratio\"]   = 0.5\n",
    "cf[\"num_test_ratio\"]    = 0.5\n",
    "cf[\"batch_size\"]        = 16\n",
    "cf[\"num_workers\"]       = 12\n",
    "cf[\"max_epochs\"]        = 50\n",
    "cf[\"accelerator\"]       = \"gpu\" if torch.cuda.is_available() else \"cpu\"\n",
    "cf[\"num_data\"]          = 1000\n",
    "cf[\"fast_dev_run\"]      = False\n",
    "cf[\"log_every_n_steps\"] = cf[\"batch_size\"] // 2\n",
    "\n",
    "# model hyperparameters\n",
    "cf[\"loss_function\"]  = nn.BCEWithLogitsLoss()\n",
    "cf[\"optimizer\"]      = optim.Adam\n",
    "cf[\"learning_rate\"]  = 1E-3\n",
    "\n",
    "# 2PCNN hyperparameters\n",
    "cf[\"gnn_layers\"] = 1\n",
    "cf[\"mlp_layers\"] = 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Module\n",
    "\n",
    "In this project, we train with data containing only the four momentum of particles. In order to reduce the size of the data (due to the long training time for quantum machine learning), we reduce the size of data by `fastjet` package by clustering particles again by `anti-kt algorithm` with smaller radius.\n",
    "\n",
    "The detail (source code) for creating fastjet reclustering events is in the `d_hep_data` file.\n",
    "\n",
    "To test the power of QML for learning space structure of data (geometric angles, e.g. $p_t$, $\\eta$, $\\phi$), we will use four momentum only (or z-boosted invariant variables $p_t$, $\\eta$, $\\phi$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataLog: Successfully create ZprimeToZhToZinvhbb with 2980 events.\n",
      "DataLog: Finish reclustering ZprimeToZhToZinvhbb with anti-kt algorithm.\n",
      "DataLog: Successfully create QCD_HT2000toInf with 4917 events.\n",
      "DataLog: Finish reclustering QCD_HT2000toInf with anti-kt algorithm.\n"
     ]
    }
   ],
   "source": [
    "def get_events_dict(channel):\n",
    "    jet_events     = JetEvents(cf[f\"{channel}_channel\"], cf[\"num_events\"], cf[\"jet_type\"], cf[\"cut\"])\n",
    "    fastjet_events = jet_events.fastjet_events(R=cf[\"subjet_radius\"])\n",
    "    events_dict = {\n",
    "        # parent jet features\n",
    "        \"e\":jet_events.events[f\"{cf['jet_type']}_e\"],\n",
    "        \"pt\":jet_events.events[f\"{cf['jet_type']}_pt\"],\n",
    "        \"eta\":jet_events.events[f\"{cf['jet_type']}_eta\"],\n",
    "        \"phi\":jet_events.events[f\"{cf['jet_type']}_phi\"],\n",
    "        \"px\":jet_events.events[f\"{cf['jet_type']}_px\"],\n",
    "        \"py\":jet_events.events[f\"{cf['jet_type']}_py\"],\n",
    "        \"pz\":jet_events.events[f\"{cf['jet_type']}_pz\"],\n",
    "        # fastjet daughter features\n",
    "        \"_e\":fastjet_events[\"e\"],\n",
    "        \"_pt\":fastjet_events[\"pt\"],\n",
    "        \"_eta\":fastjet_events[\"eta\"],\n",
    "        \"_phi\":fastjet_events[\"phi\"],\n",
    "        \"_theta\":fastjet_events[\"theta\"],\n",
    "        \"_px\":fastjet_events[\"px\"],\n",
    "        \"_py\":fastjet_events[\"py\"],\n",
    "        \"_pz\":fastjet_events[\"pz\"],\n",
    "        \"_delta_eta\":fastjet_events[\"delta_eta\"],\n",
    "        \"_delta_phi\":fastjet_events[\"delta_phi\"],\n",
    "    }\n",
    "    events_dict[\"p\"]     = (events_dict[\"px\"]**2 + events_dict[\"py\"]**2 + events_dict[\"pz\"]**2) ** 0.5\n",
    "    events_dict[\"theta\"] = np.arccos(events_dict[\"pz\"]/events_dict[\"p\"])\n",
    "    return events_dict\n",
    "\n",
    "sig_events_dict = get_events_dict(\"sig\")\n",
    "bkg_events_dict = get_events_dict(\"bkg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ak_to_torch(arrays):\n",
    "    x = []\n",
    "    num_events = len(arrays[0])\n",
    "    for i in range(num_events):\n",
    "        tensors = [torch.tensor(a[i], dtype=torch.float32).reshape(-1, 1) for a in arrays]\n",
    "        x.append(torch.cat(tensors, axis=1))\n",
    "    return x\n",
    "\n",
    "def events_pt_eta_phi(events_dict, norm):\n",
    "    if norm:\n",
    "        arrays = ak.broadcast_arrays(\n",
    "            4 * np.arctan(events_dict[\"_pt\"] / events_dict[\"pt\"]),\n",
    "            4 * np.arctan(events_dict[\"_delta_eta\"]),\n",
    "            2 * events_dict[\"_delta_phi\"],\n",
    "        )\n",
    "    else:\n",
    "        arrays = ak.broadcast_arrays(\n",
    "            events_dict[\"_pt\"],\n",
    "            events_dict[\"_delta_eta\"],\n",
    "            events_dict[\"_delta_phi\"],\n",
    "        )\n",
    "    return ak_to_torch(arrays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JetDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, events_func, tune_batch=False, **kwargs):\n",
    "        '''Add a \"_\" prefix if it is a fastjet feature'''\n",
    "        super().__init__()\n",
    "        self.batch_size = 1 if tune_batch else cf[\"batch_size\"]\n",
    "\n",
    "        # jet events\n",
    "        sig_events = events_func(sig_events_dict, **kwargs)\n",
    "        bkg_events = events_func(bkg_events_dict, **kwargs)\n",
    "        self.sig_data_list = self._create_data_list(sig_events, 1)\n",
    "        self.bkg_data_list = self._create_data_list(bkg_events, 0)\n",
    "\n",
    "        # count the number of training, and testing\n",
    "        assert len(self.sig_data_list) >= cf[\"num_data\"], f\"sig data not enough: {len(self.sig_data_list)} < {cf['num_data']}\"\n",
    "        assert len(self.bkg_data_list) >= cf[\"num_data\"], f\"bkg data not enough: {len(self.bkg_data_list)} < {cf['num_data']}\"\n",
    "        num_train = int(cf[\"num_data\"] * cf[\"num_train_ratio\"])\n",
    "        num_test  = int(cf[\"num_data\"] * cf[\"num_test_ratio\"])\n",
    "        print(f\"DataLog: {cf['sig_channel']} has {len(self.sig_data_list)} events and {cf['bkg_channel']} has {len(self.bkg_data_list)} events.\")\n",
    "        print(f\"Choose num_data for each channel to be {cf['num_data']} | Each channel  has num_train = {num_train}, num_test = {num_test}\")\n",
    "\n",
    "        # prepare dataset for dataloader\n",
    "        train_idx = num_train\n",
    "        test_idx  = num_train + num_test\n",
    "        self.train_dataset = self.sig_data_list[:train_idx] + self.bkg_data_list[:train_idx]\n",
    "        self.test_dataset  = self.sig_data_list[train_idx:test_idx] + self.bkg_data_list[train_idx:test_idx]\n",
    "    \n",
    "    def _create_data_list(self, events, y):\n",
    "        # create pytorch_geometric \"Data\" object\n",
    "        data_list = []\n",
    "        for i in range(len(events)):\n",
    "            x = events[i]\n",
    "            edge_index = list(product(range(len(x)), range(len(x))))\n",
    "            edge_index = torch.tensor(edge_index).transpose(0, 1)\n",
    "            x.requires_grad, edge_index.requires_grad = False, False\n",
    "            data_list.append(Data(x=x, edge_index=edge_index, y=y))\n",
    "        random.shuffle(data_list)\n",
    "        return data_list\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size, num_workers=cf[\"num_workers\"],  shuffle=True)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_dataset, batch_size=self.batch_size, num_workers=cf[\"num_workers\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models\n",
    "\n",
    "To compare classical GNN with quantum GNN, we use `GraphConv` and `MessagePassing` with `pennylane` for classical and quantum repectively.\n",
    "\n",
    "- Why using `nn.ModuleList` instead of `nn.Sequential`?\n",
    "Both `nn.ModuleList` and `nn.Sequential` trace the trainable parameters autometically. However, since we are using \"gnn\" layers, we need to feed into additional argument `edge_index`. In order to check whether we are using \"gnn\" layers or not, we use `isinstance` to check the class type (Since all PyTorch Geometric graph layer inherit the class `MessagePassing`). For detail, see [When should I use nn.ModuleList and when should I use nn.Sequential?](https://discuss.pytorch.org/t/when-should-i-use-nn-modulelist-and-when-should-i-use-nn-sequential/5463/3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassicalMLP(nn.Module):\n",
    "    def __init__(self, in_channel, out_channel, hidden_channel, num_layers):\n",
    "        super().__init__()\n",
    "        if num_layers == 0:\n",
    "            self.net = nn.Linear(in_channel, out_channel)\n",
    "        else:\n",
    "            net = [nn.Linear(in_channel, hidden_channel), nn.ReLU()]\n",
    "            for _ in range(num_layers-2):\n",
    "                net += [nn.Linear(hidden_channel, hidden_channel), nn.ReLU()]\n",
    "            net += [nn.Linear(hidden_channel, out_channel)]\n",
    "            self.net = nn.Sequential(*net)\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "    \n",
    "class QuantumMLP(nn.Module):\n",
    "    def __init__(self, num_qubits, num_layers, num_reupload, measurements):\n",
    "        super().__init__()\n",
    "        # create a quantum MLP\n",
    "        @qml.qnode(qml.device('default.qubit', wires=num_qubits))\n",
    "        def circuit(inputs, weights):\n",
    "            for i in range(num_reupload):\n",
    "                qml.AngleEmbedding(features=inputs, wires=range(num_qubits), rotation='Y')\n",
    "                qml.StronglyEntanglingLayers(weights=weights[i], wires=range(num_qubits))\n",
    "            measurements_dict = {\"X\":qml.PauliX, \"Y\":qml.PauliY, \"Z\":qml.PauliZ}\n",
    "            return [qml.expval(measurements_dict[m[1]](wires=m[0])) for m in measurements]\n",
    "        # turn the quantum circuit into a torch layer\n",
    "        weight_shapes = {\"weights\":(num_reupload, num_layers, num_qubits, 3)}\n",
    "        net = [qml.qnn.TorchLayer(circuit, weight_shapes=weight_shapes)]\n",
    "        self.net = nn.Sequential(*net)\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classical 2PCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classical2PCNNForwardMP(MessagePassing):\n",
    "    def __init__(self, num_features, num_layers, aggr):\n",
    "        super().__init__(aggr=aggr)\n",
    "        self.mp_phi = ClassicalMLP(\n",
    "            in_channel     = 2*num_features, \n",
    "            out_channel    = num_features,\n",
    "            hidden_channel = 2*num_features,\n",
    "            num_layers     = num_layers,\n",
    "            )\n",
    "        self.mp_gamma = ClassicalMLP(\n",
    "            in_channel     = 2*num_features, \n",
    "            out_channel    = num_features,\n",
    "            hidden_channel = 2*num_features,\n",
    "            num_layers     = num_layers,\n",
    "        )\n",
    "    def forward(self, x, edge_index):\n",
    "        return self.propagate(edge_index, x=x)\n",
    "    def message(self, x_i, x_j):\n",
    "        return self.mp_phi(torch.cat((x_i, x_j), dim=-1))\n",
    "    def update(self, aggr_out, x):\n",
    "        return self.mp_gamma(torch.cat((x, aggr_out), dim=-1))\n",
    "\n",
    "class Classical2PCNNForward(nn.Module):\n",
    "    def __init__(self, gnn_in, gnn_layers, gnn_aggr, mlp_in, mlp_out, mlp_hidden, mlp_layers):\n",
    "        super().__init__()\n",
    "        self.gnn = Classical2PCNNForwardMP(gnn_in, gnn_layers, gnn_aggr)\n",
    "        self.mlp = ClassicalMLP(mlp_in, mlp_out, mlp_hidden, mlp_layers)\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        x = self.gnn(x, edge_index)\n",
    "        x = geom_nn.global_mean_pool(x, batch)\n",
    "        x = self.mlp(x)\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lightning Module\n",
    "\n",
    "Most of the hyperparameters are defined at `cf` configuration dictionary.\n",
    "\n",
    "Note that when using `nn.BCEWithLogitsLoss`, the first argument should not be paased to `sigmoid`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LitModel(L.LightningModule):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters(ignore=['model'])\n",
    "        self.model = model\n",
    "        self.loss_function = cf[\"loss_function\"]\n",
    "\n",
    "    def forward(self, data):\n",
    "        # predict y\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "        x = self.model(x, edge_index, batch)\n",
    "        x = x.squeeze(dim=-1)\n",
    "\n",
    "        # calculate loss and accuracy\n",
    "        y_pred = x > 0\n",
    "        y_true = data.y\n",
    "        loss   = self.loss_function(x, y_true.float())\n",
    "        acc    = (y_pred == data.y).float().mean()\n",
    "        return loss, acc\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = cf[\"optimizer\"](self.parameters(), lr=cf[\"learning_rate\"])\n",
    "        return optimizer\n",
    "\n",
    "    def training_step(self, data, batch_idx):\n",
    "        loss, acc = self.forward(data)\n",
    "        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, batch_size=len(data.x))\n",
    "        self.log(\"train_acc\", acc, on_step=True, on_epoch=True, batch_size=len(data.x))\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, data, batch_idx):\n",
    "        _, acc = self.forward(data)\n",
    "        self.log(\"test_acc\", acc, on_step=True, on_epoch=True, batch_size=len(data.x))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Test the Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data_module, tune_batch=False, suffix=\"\"):\n",
    "    # setup id and path for saving\n",
    "    project  = cf['project']\n",
    "    group    = f\"{cf['num_data']}_{cf['sig_channel']}_{cf['bkg_channel']}_{cf['jet_type']}_R{cf['subjet_radius']}\"\n",
    "    job_type = model.__class__.__name__\n",
    "    name     = f\"{job_type}_{cf['time']}_{suffix}\"\n",
    "    id       = f\"{group}_{name}\"\n",
    "    root_dir = f\"./result\"\n",
    "    if not os.path.isdir(root_dir):\n",
    "        os.makedirs(root_dir)\n",
    "\n",
    "    # wandb logger setup\n",
    "    if cf[\"wandb\"]:\n",
    "        try:\n",
    "            # check runs\n",
    "            api = wandb.Api()\n",
    "            for run in api.runs(f\"ntuyianchen/{project}\"):\n",
    "                if f\"{group}_{job_type}\" in run.id and suffix in run.id:\n",
    "                    run = api.run(f\"ntuyianchen/{project}/\"+run.id)\n",
    "                    run.delete()\n",
    "        except ValueError as e:\n",
    "            print(e)\n",
    "            print(f\"Now creating new project {project}\")\n",
    "        wandb_logger = WandbLogger(project=project, group=group, job_type=job_type, name=name, id=id, save_dir=root_dir)\n",
    "        wandb_logger.experiment.config.update(cf)\n",
    "        wandb_logger.watch(model, log=\"all\")\n",
    "\n",
    "    # start lightning training\n",
    "    logger   = wandb_logger if cf[\"wandb\"] else None\n",
    "    trainer  = L.Trainer(\n",
    "        logger=logger, \n",
    "        accelerator       = cf[\"accelerator\"], \n",
    "        max_epochs        = cf[\"max_epochs\"], \n",
    "        fast_dev_run      = cf[\"fast_dev_run\"],\n",
    "        log_every_n_steps = cf[\"log_every_n_steps\"],\n",
    "        )\n",
    "    litmodel = LitModel(model)\n",
    "    \n",
    "    if tune_batch:\n",
    "        tuner = Tuner(trainer)\n",
    "        tuner.scale_batch_size(litmodel, datamodule=data_module, mode=\"binsearch\")\n",
    "    trainer.fit(litmodel, datamodule=data_module)\n",
    "    trainer.test(litmodel, datamodule=data_module)\n",
    "\n",
    "    # finish wandb monitoring\n",
    "    if cf[\"wandb\"]:\n",
    "        wandb.finish()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start training each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataLog: ZprimeToZhToZinvhbb has 2980 events and QCD_HT2000toInf has 4917 events.\n",
      "Choose num_data for each channel to be 1000 | Each channel  has num_train = 500, num_test = 500\n",
      "DataLog: ZprimeToZhToZinvhbb has 2980 events and QCD_HT2000toInf has 4917 events.\n",
      "Choose num_data for each channel to be 1000 | Each channel  has num_train = 500, num_test = 500\n",
      "Could not find project t_4vec_2pcnn\n",
      "Now creating new project t_4vec_2pcnn\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.14.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>./result/wandb/run-20230406_224842-1000_ZprimeToZhToZinvhbb_QCD_HT2000toInf_fatjet_R0.25_Classical2PCNNForward_20230406_224825_pt_eta_phi</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ntuyianchen/t_4vec_2pcnn/runs/1000_ZprimeToZhToZinvhbb_QCD_HT2000toInf_fatjet_R0.25_Classical2PCNNForward_20230406_224825_pt_eta_phi' target=\"_blank\">Classical2PCNNForward_20230406_224825_pt_eta_phi</a></strong> to <a href='https://wandb.ai/ntuyianchen/t_4vec_2pcnn' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ntuyianchen/t_4vec_2pcnn' target=\"_blank\">https://wandb.ai/ntuyianchen/t_4vec_2pcnn</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ntuyianchen/t_4vec_2pcnn/runs/1000_ZprimeToZhToZinvhbb_QCD_HT2000toInf_fatjet_R0.25_Classical2PCNNForward_20230406_224825_pt_eta_phi' target=\"_blank\">https://wandb.ai/ntuyianchen/t_4vec_2pcnn/runs/1000_ZprimeToZhToZinvhbb_QCD_HT2000toInf_fatjet_R0.25_Classical2PCNNForward_20230406_224825_pt_eta_phi</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: logging graph, to disable use `wandb.watch(log_graph=False)`\n",
      "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
      "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
      "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
      "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
      "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "INFO:lightning.pytorch.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "INFO: \n",
      "  | Name          | Type                  | Params\n",
      "--------------------------------------------------------\n",
      "0 | model         | Classical2PCNNForward | 172   \n",
      "1 | loss_function | BCEWithLogitsLoss     | 0     \n",
      "--------------------------------------------------------\n",
      "172       Trainable params\n",
      "0         Non-trainable params\n",
      "172       Total params\n",
      "0.001     Total estimated model params size (MB)\n",
      "INFO:lightning.pytorch.callbacks.model_summary:\n",
      "  | Name          | Type                  | Params\n",
      "--------------------------------------------------------\n",
      "0 | model         | Classical2PCNNForward | 172   \n",
      "1 | loss_function | BCEWithLogitsLoss     | 0     \n",
      "--------------------------------------------------------\n",
      "172       Trainable params\n",
      "0         Non-trainable params\n",
      "172       Total params\n",
      "0.001     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 63/63 [00:00<00:00, 77.99it/s, v_num=_phi]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_lightning.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=50` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 63/63 [00:00<00:00, 77.54it/s, v_num=_phi]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "INFO:lightning.pytorch.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 63/63 [00:00<00:00, 246.19it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      test_acc_epoch       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.6815106868743896     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m     test_acc_epoch      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.6815106868743896    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>test_acc_epoch</td><td>▁</td></tr><tr><td>test_acc_step</td><td>▃▄▄▂▂▁▆▃▅▃▃▃▄▁▂▃▃▃▃▃▅▇▇██▇███▇▅▇▇▇█▇▇▇█▆</td></tr><tr><td>train_acc_epoch</td><td>▁▁▂▃▃▄▄▄▃▄▄▄▅▅▆▅▆▆▆▇▇▆▆▆█▇▇▇▅▇▇▇▇▆███▇▇▇</td></tr><tr><td>train_acc_step</td><td>▅▄▁▇▄▅▅▃▅▅▄▄▄▇▂▅▂▅▃▅▃▅▅▅▅▂█▃▄▅▁▆▄▄▅▅▇▅▄▅</td></tr><tr><td>train_loss_epoch</td><td>██▇▇▆▆▅▅▅▅▅▄▄▄▄▄▃▃▃▂▃▃▃▃▂▂▁▂▃▂▁▁▁▂▁▁▁▂▁▁</td></tr><tr><td>train_loss_step</td><td>▅▄▆▄▄▃▄▄▄▄▅▄▅▃▆▄▅▄▅▄▅▃▃▃▄█▂▄▅▄▇▃▄▄▃▃▁▃▅▄</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇███▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>50</td></tr><tr><td>test_acc_epoch</td><td>0.68151</td></tr><tr><td>test_acc_step</td><td>0.75</td></tr><tr><td>train_acc_epoch</td><td>0.70593</td></tr><tr><td>train_acc_step</td><td>0.6875</td></tr><tr><td>train_loss_epoch</td><td>0.56518</td></tr><tr><td>train_loss_step</td><td>0.56832</td></tr><tr><td>trainer/global_step</td><td>3150</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Classical2PCNNForward_20230406_224825_pt_eta_phi</strong> at: <a href='https://wandb.ai/ntuyianchen/t_4vec_2pcnn/runs/1000_ZprimeToZhToZinvhbb_QCD_HT2000toInf_fatjet_R0.25_Classical2PCNNForward_20230406_224825_pt_eta_phi' target=\"_blank\">https://wandb.ai/ntuyianchen/t_4vec_2pcnn/runs/1000_ZprimeToZhToZinvhbb_QCD_HT2000toInf_fatjet_R0.25_Classical2PCNNForward_20230406_224825_pt_eta_phi</a><br/>Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./result/wandb/run-20230406_224842-1000_ZprimeToZhToZinvhbb_QCD_HT2000toInf_fatjet_R0.25_Classical2PCNNForward_20230406_224825_pt_eta_phi/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.14.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>./result/wandb/run-20230406_224932-1000_ZprimeToZhToZinvhbb_QCD_HT2000toInf_fatjet_R0.25_Classical2PCNNForward_20230406_224825_pt_eta_phi_norm</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ntuyianchen/t_4vec_2pcnn/runs/1000_ZprimeToZhToZinvhbb_QCD_HT2000toInf_fatjet_R0.25_Classical2PCNNForward_20230406_224825_pt_eta_phi_norm' target=\"_blank\">Classical2PCNNForward_20230406_224825_pt_eta_phi_norm</a></strong> to <a href='https://wandb.ai/ntuyianchen/t_4vec_2pcnn' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ntuyianchen/t_4vec_2pcnn' target=\"_blank\">https://wandb.ai/ntuyianchen/t_4vec_2pcnn</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ntuyianchen/t_4vec_2pcnn/runs/1000_ZprimeToZhToZinvhbb_QCD_HT2000toInf_fatjet_R0.25_Classical2PCNNForward_20230406_224825_pt_eta_phi_norm' target=\"_blank\">https://wandb.ai/ntuyianchen/t_4vec_2pcnn/runs/1000_ZprimeToZhToZinvhbb_QCD_HT2000toInf_fatjet_R0.25_Classical2PCNNForward_20230406_224825_pt_eta_phi_norm</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: logging graph, to disable use `wandb.watch(log_graph=False)`\n",
      "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
      "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
      "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
      "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
      "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "INFO:lightning.pytorch.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "INFO: \n",
      "  | Name          | Type                  | Params\n",
      "--------------------------------------------------------\n",
      "0 | model         | Classical2PCNNForward | 172   \n",
      "1 | loss_function | BCEWithLogitsLoss     | 0     \n",
      "--------------------------------------------------------\n",
      "172       Trainable params\n",
      "0         Non-trainable params\n",
      "172       Total params\n",
      "0.001     Total estimated model params size (MB)\n",
      "INFO:lightning.pytorch.callbacks.model_summary:\n",
      "  | Name          | Type                  | Params\n",
      "--------------------------------------------------------\n",
      "0 | model         | Classical2PCNNForward | 172   \n",
      "1 | loss_function | BCEWithLogitsLoss     | 0     \n",
      "--------------------------------------------------------\n",
      "172       Trainable params\n",
      "0         Non-trainable params\n",
      "172       Total params\n",
      "0.001     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 63/63 [00:00<00:00, 80.52it/s, v_num=norm]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_lightning.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=50` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 63/63 [00:00<00:00, 80.04it/s, v_num=norm]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "INFO:lightning.pytorch.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 63/63 [00:00<00:00, 227.36it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      test_acc_epoch       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.5890228152275085     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m     test_acc_epoch      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.5890228152275085    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>test_acc_epoch</td><td>▁</td></tr><tr><td>test_acc_step</td><td>▅▅▄▄▅▄▇▅▅▆█▅▅▆▅▅▅▇▅▅▃▂▂▃▂▄▅▅▁▂▂▃▂▂▅▇▂▃▅▅</td></tr><tr><td>train_acc_epoch</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▃▄▆▆▆▆▆▇▆▇▇▇▇▇▇▇▆▇▇██▇▆▇██</td></tr><tr><td>train_acc_step</td><td>▆▅▅▅▅▃▅▄▄▅▅▄▃▂▁▄▅▆▆▇█▄▆▇▄▇▆▆▄▇▅▅▄▆▆▅▅█▄▅</td></tr><tr><td>train_loss_epoch</td><td>█▆▆▅▅▅▅▅▄▄▄▄▄▄▄▄▄▃▃▃▃▃▃▃▃▂▃▂▂▂▂▂▂▂▂▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>▃▆▄▅▅▇▅▆▆▄▅▅▆▇▇▆▆▄▅▄▄▇▅▂▆▅▄▅▆▄▆▅▅▃▄▅▃▁▅█</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇███▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>50</td></tr><tr><td>test_acc_epoch</td><td>0.58902</td></tr><tr><td>test_acc_step</td><td>0.375</td></tr><tr><td>train_acc_epoch</td><td>0.60556</td></tr><tr><td>train_acc_step</td><td>0.75</td></tr><tr><td>train_loss_epoch</td><td>0.66874</td></tr><tr><td>train_loss_step</td><td>0.61029</td></tr><tr><td>trainer/global_step</td><td>3150</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Classical2PCNNForward_20230406_224825_pt_eta_phi_norm</strong> at: <a href='https://wandb.ai/ntuyianchen/t_4vec_2pcnn/runs/1000_ZprimeToZhToZinvhbb_QCD_HT2000toInf_fatjet_R0.25_Classical2PCNNForward_20230406_224825_pt_eta_phi_norm' target=\"_blank\">https://wandb.ai/ntuyianchen/t_4vec_2pcnn/runs/1000_ZprimeToZhToZinvhbb_QCD_HT2000toInf_fatjet_R0.25_Classical2PCNNForward_20230406_224825_pt_eta_phi_norm</a><br/>Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./result/wandb/run-20230406_224932-1000_ZprimeToZhToZinvhbb_QCD_HT2000toInf_fatjet_R0.25_Classical2PCNNForward_20230406_224825_pt_eta_phi_norm/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# classical 2pcnn\n",
    "data_pt_eta_phi = JetDataModule(events_func=events_pt_eta_phi, norm=False)\n",
    "data_pt_eta_phi_norm = JetDataModule(events_func=events_pt_eta_phi, norm=True)\n",
    "input_dim = data_pt_eta_phi.train_dataset[0].x.shape[1]\n",
    "cf_2pcnn = {\n",
    "    \"gnn_in\":input_dim, \n",
    "    \"gnn_layers\":cf[\"gnn_layers\"],\n",
    "    \"gnn_aggr\":\"add\", \n",
    "    \"mlp_in\":input_dim,\n",
    "    \"mlp_out\":1, \n",
    "    \"mlp_hidden\":3*input_dim, \n",
    "    \"mlp_layers\":cf[\"mlp_layers\"],\n",
    "}\n",
    "train(Classical2PCNNForward(**cf_2pcnn), data_module=data_pt_eta_phi, suffix=\"pt_eta_phi\")\n",
    "train(Classical2PCNNForward(**cf_2pcnn), data_module=data_pt_eta_phi_norm, suffix=\"pt_eta_phi_norm\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "abdd0d95ca50f233d1202cce1ba28eab5ada50f7ec17823ef40ef9b79347f6f5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
