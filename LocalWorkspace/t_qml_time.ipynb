{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic\n",
    "import os, time, itertools, subprocess\n",
    "\n",
    "# qml\n",
    "import pennylane as qml\n",
    "from pennylane import numpy as np\n",
    "\n",
    "# pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# pytorch_lightning\n",
    "import lightning as L\n",
    "import lightning.pytorch as pl\n",
    "\n",
    "# wandb\n",
    "import wandb\n",
    "from lightning.pytorch.loggers import WandbLogger\n",
    "wandb.login()\n",
    "\n",
    "# reproducibility\n",
    "L.seed_everything(3020616)\n",
    "\n",
    "# faster calculation on GPU but less precision\n",
    "torch.set_float32_matmul_precision(\"medium\")\n",
    "\n",
    "# current time\n",
    "global_time = input(\"Input time if needed:\")\n",
    "if global_time == \"\":\n",
    "    global_time = time.strftime(\"%Y%m%d_%H%M%S\", time.localtime())\n",
    "\n",
    "gpu_name = \"NVIDIA_RTX_4090\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QML NN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, q_device, q_diff, q_interface, num_qubits, num_reupload, num_qlayers, num_clayers, num_chidden):\n",
    "        super().__init__()\n",
    "    \n",
    "        # create a quantum MLP\n",
    "        @qml.qnode(qml.device(q_device, wires=num_qubits), diff_method=q_diff, interface=q_interface)\n",
    "        def circuit(inputs, weights):\n",
    "            for i in range(num_reupload):\n",
    "                qml.AngleEmbedding(features=inputs, wires=range(num_qubits), rotation='Y')\n",
    "                qml.StronglyEntanglingLayers(weights=weights[i], wires=range(num_qubits))\n",
    "            return [qml.expval(qml.PauliZ(wires=i)) for i in range(num_qubits)]\n",
    "        \n",
    "        # turn the quantum circuit into a torch layer\n",
    "        weight_shapes = {\"weights\":(num_reupload, num_qlayers, num_qubits, 3)}\n",
    "        q_net = [qml.qnn.TorchLayer(circuit, weight_shapes=weight_shapes)]\n",
    "\n",
    "        # classical mlp\n",
    "        c_in, c_hidden, c_out = num_qubits, num_chidden, 1\n",
    "        c_net = [nn.Linear(c_in, c_hidden), nn.ReLU()]\n",
    "        for _ in range(num_clayers-2):\n",
    "            c_net += [nn.Linear(c_hidden, c_hidden), nn.ReLU()]\n",
    "        c_net += [nn.Linear(c_hidden, c_out)]\n",
    "\n",
    "        # combine classical and quantum net\n",
    "        self.net = nn.Sequential(*(q_net + c_net))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lit Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LitModel(L.LightningModule):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters(ignore=['model'])\n",
    "        self.model = model\n",
    "        self.loss_function = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    def forward(self, data):\n",
    "        # predict y\n",
    "        x, y_true = data\n",
    "        x = self.model(x)\n",
    "        x = x.squeeze(dim=-1)\n",
    "\n",
    "        # calculate loss and accuracy\n",
    "        y_pred = x > 0\n",
    "        loss   = self.loss_function(x, y_true.float())\n",
    "        acc    = (y_pred == y_true).float().mean()\n",
    "        return loss, acc\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.parameters(), lr=1E-3)\n",
    "        return optimizer\n",
    "    \n",
    "    def on_train_epoch_start(self):\n",
    "        self.start_time = time.time()\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        self.end_time = time.time()\n",
    "        delta_time = self.end_time - self.start_time\n",
    "        self.log(\"epoch_time\", delta_time, on_step=False, on_epoch=True)\n",
    "\n",
    "    def training_step(self, data, batch_idx):\n",
    "        loss, acc = self.forward(data)\n",
    "        # self.log(\"train_loss\", loss, on_step=True, on_epoch=True)\n",
    "        # self.log(\"train_acc\", acc, on_step=True, on_epoch=True)\n",
    "        return loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomDataset(Dataset):\n",
    "    def __init__(self, num_data, num_dim):\n",
    "        super().__init__()\n",
    "        self.x = torch.rand(num_data, num_dim)\n",
    "        self.y = torch.cat((torch.ones(num_data//2), torch.zeros(num_data//2)))\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]\n",
    "\n",
    "class RandomDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, num_data, num_dim, batch_size, num_workers):\n",
    "        super().__init__()\n",
    "        self.train_dataset  = RandomDataset(num_data, num_dim)\n",
    "        self.batch_size     = batch_size\n",
    "        self.num_workers    = num_workers\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size, num_workers=self.num_workers, shuffle=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_time(wb, c_device, model_config, data_config):\n",
    "    model       = MLP(**model_config)\n",
    "    litmodel    = LitModel(model)\n",
    "    data_module = RandomDataModule(**data_config)\n",
    "\n",
    "    mcf = model_config\n",
    "    dcf = data_config\n",
    "    if wb == True:\n",
    "        group    = f\"{global_time}_{gpu_name}\"\n",
    "        job_type = f\"{c_device}|{mcf['q_device']}|diff_{mcf['q_diff']}|interface_{mcf['q_interface']}\"\n",
    "        name     = f\"{job_type}|batch{dcf['batch_size']}|worker{dcf['num_workers']}|dim{dcf['num_dim']}\"\n",
    "        id       = f\"{group}|{name}\"\n",
    "\n",
    "        # wandb logger\n",
    "        wandb_logger = WandbLogger(project=\"t_qml_time\", group=group, job_type=job_type, name=name, id=id, save_dir=f\"./result\")\n",
    "        wandb_logger.experiment.config.update(mcf)\n",
    "        wandb_logger.experiment.config.update(dcf)\n",
    "        wandb_logger.watch(model, log=\"all\")\n",
    "        logger = wandb_logger\n",
    "    else:\n",
    "        logger = None\n",
    "\n",
    "    trainer  = L.Trainer(\n",
    "        logger      = logger, \n",
    "        accelerator = c_device, \n",
    "        max_epochs  = 3,\n",
    "        log_every_n_steps = 1,\n",
    "        )\n",
    "\n",
    "    print(f\"Start testing {c_device}|{mcf['q_device']}|diff_{mcf['q_diff']}|interface_{mcf['q_interface']}|batch{dcf['batch_size']}|worker{dcf['num_workers']}|dim{dcf['num_dim']}\")\n",
    "    trainer.fit(litmodel, datamodule=data_module)\n",
    "\n",
    "    if wb:\n",
    "        wandb.finish()\n",
    "        print(\"#\" * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_test = True\n",
    "\n",
    "if real_test:\n",
    "    wb = True\n",
    "    l_num_dim     = [20]\n",
    "    l_cpu_batch   = [4, 8]\n",
    "    l_gpu_batch   = [4, 8]\n",
    "    l_num_workers = [0]\n",
    "    l_cpu_product = list(itertools.product([\"cpu\"], l_num_dim, l_cpu_batch, l_num_workers))\n",
    "    l_gpu_product = list(itertools.product([\"gpu\"], l_num_dim, l_gpu_batch, l_num_workers))\n",
    "    l_product     = l_cpu_product + l_gpu_product\n",
    "\n",
    "else:\n",
    "    wb = False\n",
    "    l_num_dim     = [20]\n",
    "    l_cpu_batch   = [4, 8]\n",
    "    l_gpu_batch   = [4, 8]\n",
    "    l_num_workers = [0]\n",
    "    l_cpu_product = list(itertools.product([\"cpu\"], l_num_dim, l_cpu_batch, l_num_workers))\n",
    "    l_gpu_product = list(itertools.product([\"gpu\"], l_num_dim, l_gpu_batch, l_num_workers))\n",
    "    l_product     = l_gpu_product\n",
    "\n",
    "q_tuple = [\n",
    "    # (q_device       , q_diff    , q_interface)\n",
    "    (\"default.qubit\"  , \"best\"    , \"auto\"),\n",
    "    (\"default.qubit\"  , \"best\"    , \"torch\"),\n",
    "    (\"lightning.qubit\", \"adjoint\" , \"auto\"),\n",
    "    (\"lightning.qubit\", \"adjoint\" , \"torch\"),\n",
    "    (\"lightning.gpu\"  , \"adjoint\" , \"auto\"),\n",
    "    (\"lightning.gpu\"  , \"adjoint\" , \"torch\"),\n",
    "]\n",
    "\n",
    "for c_device, num_dim, batch_size, num_workers in l_product:\n",
    "    for q_device, q_diff, q_interface in q_tuple:\n",
    "        model_config = {\n",
    "            \"q_device\"     : q_device,\n",
    "            \"q_diff\"       : q_diff,\n",
    "            \"q_interface\"  : q_interface,\n",
    "            \"num_qubits\"   : num_dim,\n",
    "            \"num_reupload\" : 2,\n",
    "            \"num_qlayers\"  : 1,\n",
    "            \"num_clayers\"  : 2,\n",
    "            \"num_chidden\"  : 4 * num_dim,\n",
    "        }\n",
    "        data_config = {\n",
    "            \"num_data\"    : 16,\n",
    "            \"num_dim\"     : num_dim,\n",
    "            \"batch_size\"  : batch_size,\n",
    "            \"num_workers\" : num_workers,\n",
    "        }\n",
    "        test_time(wb, c_device, model_config, data_config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
