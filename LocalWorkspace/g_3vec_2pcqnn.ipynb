{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic packages\n",
    "import os, time, random\n",
    "from itertools import product\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# data\n",
    "import awkward as ak\n",
    "import d_hep_data\n",
    "\n",
    "# model\n",
    "import m_nn\n",
    "import m_lightning\n",
    "\n",
    "# qml\n",
    "import pennylane as qml\n",
    "from pennylane import numpy as np\n",
    "\n",
    "# pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# pytorch_lightning\n",
    "import lightning as L\n",
    "import lightning.pytorch as pl\n",
    "\n",
    "# pytorch_geometric\n",
    "import networkx as nx\n",
    "import torch_geometric.nn as geom_nn\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import MessagePassing\n",
    "\n",
    "# wandb\n",
    "import wandb\n",
    "from lightning.pytorch.loggers import WandbLogger\n",
    "wandb.login()\n",
    "\n",
    "# reproducibility\n",
    "L.seed_everything(3020616)\n",
    "\n",
    "# faster calculation on GPU but less precision\n",
    "torch.set_float32_matmul_precision(\"medium\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configuration dictionary\n",
    "cf = {}\n",
    "cf[\"time\"]     = time.strftime(\"%Y%m%d_%H%M%S\", time.localtime())\n",
    "cf[\"wandb\"]    = True\n",
    "cf[\"project\"]  = \"g_3vec_2pcqnn\"\n",
    "cf[\"rnd_seed\"] = None # to be determined by for loop\n",
    "\n",
    "# data infotmation\n",
    "cf[\"num_events\"]    = \"50000\"\n",
    "cf[\"sig_channel\"]   = \"ZprimeToZhToZinvhbb\"\n",
    "cf[\"bkg_channel\"]   = \"QCD_HT2000toInf\"\n",
    "cf[\"jet_type\"]      = \"fatjet\"\n",
    "cf[\"subjet_radius\"] = None # to be determined from [0.25, 0.5, 0.75]\n",
    "cf[\"cut_limit\"]     = (500, 1500)\n",
    "cf[\"bin\"]           = 10\n",
    "cf[\"num_bin_data\"]  = None # to be determined from [100, 200, 300]\n",
    "\n",
    "# traning configuration\n",
    "cf[\"num_train_ratio\"]   = 0.5\n",
    "cf[\"num_test_ratio\"]    = 0.5\n",
    "cf[\"batch_size\"]        = 64\n",
    "cf[\"num_workers\"]       = 0\n",
    "cf[\"max_epochs\"]        = 100\n",
    "cf[\"accelerator\"]       = \"cpu\"\n",
    "cf[\"fast_dev_run\"]      = False\n",
    "cf[\"log_every_n_steps\"] = cf[\"batch_size\"] // 2\n",
    "\n",
    "# model hyperparameters\n",
    "cf[\"loss_function\"]  = nn.BCEWithLogitsLoss()\n",
    "cf[\"optimizer\"]      = optim.Adam\n",
    "cf[\"learning_rate\"]  = 1E-3\n",
    "\n",
    "# 2PCNN hyperparameters\n",
    "cf[\"gnn_layers\"] = None # to be determined by grid search\n",
    "cf[\"mlp_layers\"] = None # to be determined by grid search"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JetDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, sig_buffer, bkg_buffer, normalize):\n",
    "        '''Add a \"_\" prefix if it is a fastjet feature'''\n",
    "        super().__init__()\n",
    "        # jet events\n",
    "        sig_events = sig_buffer.get_uniform_bin_data()\n",
    "        bkg_events = bkg_buffer.get_uniform_bin_data()\n",
    "        sig_events = self._get_pt_eta_phi(sig_events, normalize)\n",
    "        bkg_events = self._get_pt_eta_phi(bkg_events, normalize)\n",
    "        self.sig_data_list = self._create_data_list(sig_events, 1)\n",
    "        self.bkg_data_list = self._create_data_list(bkg_events, 0)\n",
    "\n",
    "        # count the number of training, and testing\n",
    "        num_data = cf[\"bin\"] * cf[\"num_bin_data\"]\n",
    "        assert len(self.sig_data_list) >= num_data, f\"sig data not enough: {len(self.sig_data_list)} < {num_data}\"\n",
    "        assert len(self.bkg_data_list) >= num_data, f\"bkg data not enough: {len(self.bkg_data_list)} < {num_data}\"\n",
    "        num_train = int(num_data * cf[\"num_train_ratio\"])\n",
    "        num_test  = int(num_data * cf[\"num_test_ratio\"])\n",
    "        print(f\"DataLog: {cf['sig_channel']} has {len(self.sig_data_list)} events and {cf['bkg_channel']} has {len(self.bkg_data_list)} events.\")\n",
    "        print(f\"Choose num_data for each channel to be {num_data} | Each channel  has num_train = {num_train}, num_test = {num_test}\")\n",
    "\n",
    "        # prepare dataset for dataloader\n",
    "        train_idx = num_train\n",
    "        test_idx  = num_train + num_test\n",
    "        self.train_dataset = self.sig_data_list[:train_idx] + self.bkg_data_list[:train_idx]\n",
    "        self.test_dataset  = self.sig_data_list[train_idx:test_idx] + self.bkg_data_list[train_idx:test_idx]\n",
    "\n",
    "    def _get_pt_eta_phi(self, events, normalize):\n",
    "        if normalize:\n",
    "            f1 = np.arctan(events[\"_pt\"] / events[\"pt\"])\n",
    "            f2 = np.pi / 2 * events[\"_delta_eta\"]\n",
    "            f3 = np.pi * events[\"_delta_phi\"]\n",
    "            arrays = ak.zip([f1, f2, f3])\n",
    "        else:\n",
    "            f1 = events[\"_pt\"]\n",
    "            f2 = events[\"_delta_eta\"]\n",
    "            f3 = events[\"_delta_phi\"]\n",
    "            arrays = ak.zip([f1, f2, f3])\n",
    "        arrays = arrays.to_list()\n",
    "        x = [torch.tensor(arrays[i], dtype=torch.float32) for i in range(len(arrays))]\n",
    "        return x\n",
    "    \n",
    "    def _create_data_list(self, events, y):\n",
    "        # create pytorch_geometric \"Data\" object\n",
    "        data_list = []\n",
    "        for i in range(len(events)):\n",
    "            x = events[i]\n",
    "            edge_index = list(product(range(len(x)), range(len(x))))\n",
    "            edge_index = torch.tensor(edge_index).transpose(0, 1)\n",
    "            x.requires_grad, edge_index.requires_grad = False, False\n",
    "            data_list.append(Data(x=x, edge_index=edge_index, y=y))\n",
    "        random.shuffle(data_list)\n",
    "        return data_list\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=cf[\"batch_size\"], num_workers=cf[\"num_workers\"],  shuffle=True)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_dataset, batch_size=cf[\"batch_size\"], num_workers=cf[\"num_workers\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PCNNMessagePassing(MessagePassing):\n",
    "    def __init__(self, phi, gamma, aggr):\n",
    "        super().__init__(aggr=aggr, flow=\"target_to_source\")\n",
    "        self.phi   = phi\n",
    "        self.gamma = gamma\n",
    "    def forward(self, x, edge_index):\n",
    "        return self.propagate(edge_index, x=x)\n",
    "    def message(self, x_i, x_j):\n",
    "        return self.phi(torch.cat((x_i, x_j), dim=-1))\n",
    "    def update(self, aggr_out, x):\n",
    "        # only use pt and eta in gamma\n",
    "        return self.gamma(torch.cat((x[..., :2], aggr_out), dim=-1))\n",
    "\n",
    "class Classical2PCNNForward(nn.Module):\n",
    "    def __init__(self, gnn_in, gnn_layers, gnn_aggr, mlp_in, mlp_out, mlp_hidden, mlp_layers):\n",
    "        super().__init__()\n",
    "        # GNN\n",
    "        num_features   = 3 # pt, eta, phi\n",
    "        in_channel     = num_features * 2 # 2 from 2PCNN\n",
    "        hidden_channel = \n",
    "        gnn_phi      = m_nn.ClassicalMLP(in_channel=in_channel, hidden_channel=)\n",
    "        gnn_gamma = \n",
    "        self.gnn  = m_nn.CatMessagePassing()\n",
    "\n",
    "\n",
    "        self.gnn = Classical2PCNNForwardMP(gnn_in, gnn_layers, gnn_aggr)\n",
    "        self.mlp = ClassicalMLP(mlp_in, mlp_out, mlp_hidden, mlp_layers)\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        x = self.gnn(x, edge_index)\n",
    "        x = geom_nn.global_mean_pool(x, batch)\n",
    "        x = self.mlp(x)\n",
    "        return x\n",
    "\n",
    "class Quantum2PCQNNForward(nn.Module):\n",
    "    def __init__(self, gnn_in, gnn_layers, gnn_reupload, gnn_aggr, mlp_in, mlp_out, mlp_hidden, mlp_layers):\n",
    "        super().__init__()\n",
    "        num_features   = 3 # pt, eta, phi\n",
    "        in_channel     = num_features * 2 # 2 from 2PCNN\n",
    "        gnn_phi = m_nn.QuantumMLP(num_qubits=gnn_in)\n",
    "        self.gnn = Quantum2PCQNNForwardMP(gnn_in, gnn_layers, gnn_reupload, gnn_aggr)\n",
    "        self.mlp = ClassicalMLP(mlp_in, mlp_out, mlp_hidden, mlp_layers)\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        x = self.gnn(x, edge_index)\n",
    "        x = geom_nn.global_mean_pool(x, batch)\n",
    "        x = self.mlp(x)\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Test the Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data_module, commit=\"\", suffix=\"\"):\n",
    "    # setup id and path for saving\n",
    "    project  = cf['project']\n",
    "    group    = f\"{cf['time']}_{commit}_{cf['sig_channel']}_{cf['bkg_channel']}_{cf['jet_type']}\"\n",
    "    job_type = f\"R{cf['subjet_radius']}_B{cf['bin']}_D{cf['num_bin_data']}\"\n",
    "    name     = f\"{model.__class__.__name__}_gl{cf['gnn_layers']}_ml{cf['mlp_layers']}_{suffix} | {job_type} | rnd_{cf['rnd_seed']} | {cf['time']}\"\n",
    "    id       = f\"{name}\"\n",
    "    tags     = [model.__class__.__name__, cf['sig_channel'], cf['bkg_channel'], cf['jet_type'], str(cf['subjet_radius'])]\n",
    "    root_dir = f\"./result\"\n",
    "    if not os.path.isdir(root_dir):\n",
    "        os.makedirs(root_dir)\n",
    "\n",
    "    # check whether wandb name exists\n",
    "    api     = wandb.Api()\n",
    "    project = cf[\"project\"]\n",
    "    if id in set([run.id for run in api.runs(f\"ntuyianchen/{project}\")]):\n",
    "        print(f\"{id} already exists, skip this run\")\n",
    "        return\n",
    "\n",
    "    # wandb logger setup\n",
    "    if cf[\"wandb\"]:\n",
    "        cf[\"model_structure\"] = f\"gl{cf['gnn_layers']}_ml{cf['mlp_layers']}\"\n",
    "        cf[\"model_name\"]      = model.__class__.__name__\n",
    "        cf[\"group_rnd_seed\"]  = f\"{cf['model_name']}_{cf['model_structure']}_{suffix} | {job_type}\"\n",
    "        cf[\"suffix\"]          = suffix\n",
    "        wandb_logger = WandbLogger(project=project, group=group, job_type=job_type, name=name, id=id, save_dir=root_dir, tags=tags)\n",
    "        wandb_logger.experiment.config.update(cf)\n",
    "        wandb_logger.watch(model, log=\"all\")\n",
    "\n",
    "    # start lightning training\n",
    "    logger   = wandb_logger if cf[\"wandb\"] else None\n",
    "    trainer  = L.Trainer(\n",
    "        logger=logger, \n",
    "        accelerator       = cf[\"accelerator\"],\n",
    "        max_epochs        = cf[\"max_epochs\"],\n",
    "        fast_dev_run      = cf[\"fast_dev_run\"],\n",
    "        log_every_n_steps = cf[\"log_every_n_steps\"],\n",
    "        )\n",
    "    litmodel = LitModel(model)\n",
    "    trainer.fit(litmodel, datamodule=data_module)\n",
    "    trainer.test(litmodel, datamodule=data_module)\n",
    "\n",
    "    # finish wandb monitoring\n",
    "    if cf[\"wandb\"]:\n",
    "        wandb.finish()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start training each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_train(sig_buffer, bkg_buffer, commit=\"\"):\n",
    "    # setup\n",
    "    L.seed_everything(cf[\"rnd_seed\"])\n",
    "\n",
    "    # data module\n",
    "    data_pt_eta_phi = JetDataModule(sig_buffer, bkg_buffer, events_func=events_pt_eta_phi, norm=False)\n",
    "    data_pt_eta_phi_norm = JetDataModule(sig_buffer, bkg_buffer, events_func=events_pt_eta_phi, norm=True)\n",
    "\n",
    "    # classical 2pcnn\n",
    "    input_dim = data_pt_eta_phi.train_dataset[0].x.shape[1]\n",
    "    cf_2pcnn = {\n",
    "        \"gnn_in\":input_dim, \n",
    "        \"gnn_layers\":cf[\"gnn_layers\"],\n",
    "        \"gnn_aggr\":\"add\", \n",
    "        \"mlp_in\":input_dim,\n",
    "        \"mlp_out\":1, \n",
    "        \"mlp_hidden\":3*input_dim, \n",
    "        \"mlp_layers\":cf[\"mlp_layers\"],\n",
    "    }\n",
    "    train(Classical2PCNNForward(**cf_2pcnn), data_pt_eta_phi, commit, suffix=f\"pep\")\n",
    "    train(Classical2PCNNForward(**cf_2pcnn), data_pt_eta_phi_norm, commit, suffix=f\"pep_norm\")\n",
    "\n",
    "    # quantum 2pcqnn\n",
    "    input_dim = data_pt_eta_phi_norm.train_dataset[0].x.shape[1]\n",
    "    cf_2pcqnn = {\n",
    "        \"gnn_in\":input_dim,\n",
    "        \"gnn_layers\":cf[\"gnn_layers\"],\n",
    "        \"gnn_reupload\":input_dim,\n",
    "        \"gnn_aggr\":\"add\",\n",
    "        \"mlp_in\":input_dim,\n",
    "        \"mlp_out\":1,\n",
    "        \"mlp_hidden\":3*input_dim,\n",
    "        \"mlp_layers\":cf[\"mlp_layers\"],\n",
    "    }\n",
    "    train(Quantum2PCQNNForward(**cf_2pcqnn), data_pt_eta_phi_norm, commit, suffix=f\"pep_norm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use old date to keep run break runs\n",
    "old_time = input(\"Input old time if needed else just press enter: \")\n",
    "if old_time != \"\" and old_time != None:\n",
    "    cf[\"time\"] = old_time\n",
    "\n",
    "# short description of this experiment\n",
    "commit = input(\"Commit of this experiment (short description): \")\n",
    "\n",
    "print(f\"Date = {cf['time']} | Commit = {commit}\")\n",
    "input(\"Press enter to continue\")\n",
    "for subjet_radius in [0.75, 0.5, 0.25]:\n",
    "    for num_bin_data in [50, 100]:\n",
    "        cf[\"subjet_radius\"] = subjet_radius\n",
    "        cf[\"num_bin_data\"]  = num_bin_data\n",
    "        arg_buffer = [cf[\"num_events\"], cf[\"jet_type\"], cf[\"subjet_radius\"], cf[\"cut_limit\"], cf[\"bin\"], cf[\"num_bin_data\"]]\n",
    "        sig_buffer = d_hep_data.UniformBinJetBuffer(cf[\"sig_channel\"], *arg_buffer)\n",
    "        bkg_buffer = d_hep_data.UniformBinJetBuffer(cf[\"bkg_channel\"], *arg_buffer)\n",
    "        for gl, ml in [(1,2), (1,4)]:\n",
    "            cf[\"gnn_layers\"] = gl\n",
    "            cf[\"mlp_layers\"] = ml\n",
    "            for rnd_seed in range(5):\n",
    "                cf[\"rnd_seed\"] = rnd_seed\n",
    "                grid_train(sig_buffer, bkg_buffer, commit)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "abdd0d95ca50f233d1202cce1ba28eab5ada50f7ec17823ef40ef9b79347f6f5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
